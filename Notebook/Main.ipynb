{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master Thesis Code by Anne Langerak (Erasmus University Rotterdam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JAL: an algebra for JSON Query Optimization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Data Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages and Modules\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from operator import itemgetter, attrgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Node, required for the node creation in a JSON tree\n",
    "class Node:\n",
    "    \n",
    "    def __init__(self, v, t, i): \n",
    "        \"\"\" This function creates a node object\"\"\"\n",
    "        if t == \"atomic\":\n",
    "            self.value = v\n",
    "        else:\n",
    "            # if the node is of complex type the value is simply equal to the identifier\n",
    "            self.value = i\n",
    "        self.outgoingEdge = []\n",
    "        self.type = t\n",
    "        self.identifier = i\n",
    "    \n",
    "    \n",
    "    def set_outgoingEdge(self, edge):\n",
    "        \"\"\" This function updates the outgoingEdge property given some edge\"\"\"\n",
    "        self.outgoingEdge.append(edge)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Edge, required for the edge creation in a JSON tree\n",
    "class Edge:\n",
    "    \n",
    "    def __init__(self, l, p, c, t, i, path):\n",
    "        \"\"\" This function creates an edge object\"\"\"\n",
    "        self.label = l\n",
    "        self.parent = p\n",
    "        self.child = c\n",
    "        self.type = t\n",
    "        self.identifier = i\n",
    "        self.path = path \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Tree, required for the creation of a JSON tree\n",
    "class Tree:\n",
    "    \n",
    "    def __init__(self, top): \n",
    "        \"\"\" This function initializes a JSON tree by means of a root node\"\"\"\n",
    "        self.root = top\n",
    "        self.nodes = [top]\n",
    "        self.edges = []\n",
    "        self.node_count = 1\n",
    "        self.edge_count = 0\n",
    "    \n",
    "    \n",
    "    def get_nodes(self): \n",
    "        \"\"\" This function returns all the nodes currently present in the tree\"\"\"\n",
    "        node_values = []\n",
    "        for node in self.nodes:\n",
    "            node_values.append(node.value)\n",
    "        return node_values\n",
    "    \n",
    "    \n",
    "    def get_edges(self):\n",
    "        \"\"\" This function returns all the edges currently present in the tree\"\"\"\n",
    "        edge_labels = []\n",
    "        for edge in self.edges:\n",
    "            edge_labels.append(edge.label)\n",
    "        return edge_labels\n",
    "    \n",
    "    def tree_in_document(self):\n",
    "        \"\"\" This function converst a tree into an object type representation\"\"\"\n",
    "        nodes = {}\n",
    "        edges = {}\n",
    "        labels = {}\n",
    "        \n",
    "        for node in self.nodes:\n",
    "            temp_outgoing_list = []\n",
    "            for e in node.outgoingEdge:\n",
    "                temp_outgoing_list.append(e.identifier)\n",
    "            nodes[node.identifier] = {\n",
    "                \"value\" : node.value,\n",
    "                \"outgoingEdge\" : temp_outgoing_list,\n",
    "                \"type\" : node.type,\n",
    "            }\n",
    "        \n",
    "        for edge in self.edges:\n",
    "            edges[edge.identifier] = {\n",
    "                \"label\" : edge.label,\n",
    "                \"parent\" : edge.parent.identifier,\n",
    "                \"child\" : edge.child.identifier,\n",
    "                \"type\" : edge.type, \n",
    "                \"path\": edge.path\n",
    "            }\n",
    "            \n",
    "            if edge.path in labels.keys(): \n",
    "                labels[edge.path] = labels.get(edge.path) + [edge.child.identifier]\n",
    "            else:\n",
    "                labels[edge.path] = [edge.child.identifier] \n",
    "        \n",
    "        json_doc = {\n",
    "                    \"nodes\" : nodes,\n",
    "                    \"edges\" : edges,\n",
    "                    \"labels\" : labels\n",
    "                   }\n",
    "        return json_doc\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addition(tree, parent, child, key):\n",
    "    \"\"\" This function creates nodes and/ or edges based on the input in the given tree\"\"\"\n",
    "    type_child = type(child)\n",
    "\n",
    "    if type_child == list or type_child == 'list' or isinstance(type_child, list): # node is of type list (complex)\n",
    "        child_node = Node(child, \"complex\", tree.node_count + 1) # create node object\n",
    "        edge = Edge(key, parent, child_node, \"str\", tree.edge_count + 1, key) # create edge object\n",
    "        Node.set_outgoingEdge(parent, edge) # set outgoing edge for the node\n",
    "        # update tree's counters\n",
    "        tree.nodes.append(child_node)\n",
    "        tree.node_count += 1\n",
    "        tree.edges.append(edge)\n",
    "        tree.edge_count += 1\n",
    "            \n",
    "        for i in range(0, len(child)): # add all child nodes of the node to the tree\n",
    "            sub_child = child[i]\n",
    "            type_sub_child = type(sub_child)\n",
    "            \n",
    "            if  type_sub_child in [bool, str, int, float]: # the subchild is of atomic type\n",
    "                sub_child_node = Node(sub_child, \"atomic\", tree.node_count + 1)\n",
    "                edge_path = key \n",
    "                sub_edge = Edge(i+1, child_node, sub_child_node, \"int\", tree.edge_count + 1, edge_path)\n",
    "                Node.set_outgoingEdge(child_node, sub_edge)\n",
    "                tree.nodes.append(sub_child_node)\n",
    "                tree.node_count += 1\n",
    "                tree.edges.append(sub_edge)\n",
    "                tree.edge_count += 1\n",
    "        \n",
    "            else: # it is of type list or dictionary (complex)\n",
    "                sub_child_node = Node(sub_child, \"complex\", tree.node_count + 1)\n",
    "                edge_path = key \n",
    "                sub_edge = Edge(i+1, child_node, sub_child_node, \"int\", tree.edge_count + 1, edge_path)\n",
    "                Node.set_outgoingEdge(child_node, sub_edge)\n",
    "                tree.nodes.append(sub_child_node)\n",
    "                tree.node_count += 1\n",
    "                tree.edges.append(sub_edge)\n",
    "                tree.edge_count += 1\n",
    "                \n",
    "                if type_sub_child == dict: # subchild is an object\n",
    "                    # key traversal\n",
    "                    for subkey in sub_child.keys():\n",
    "                        addition(tree, sub_child_node, sub_child.get(subkey), (edge_path + \".\" + subkey))\n",
    "                else: # subchild is of type list\n",
    "                    # index traversal\n",
    "                    for j in range(0, len(sub_child)):\n",
    "                        addition(tree, sub_child_node, sub_child[j], (edge_path))\n",
    "                        \n",
    "    if type_child == dict: # node is of type dictionary (complex)\n",
    "        child_node = Node(child, \"complex\", tree.node_count + 1)\n",
    "\n",
    "        edge = Edge(key, parent, child_node, \"str\", tree.edge_count + 1, key)\n",
    "        Node.set_outgoingEdge(parent, edge)\n",
    "        tree.nodes.append(child_node)\n",
    "        tree.node_count += 1\n",
    "        tree.edges.append(edge)\n",
    "        tree.edge_count += 1\n",
    "        parent_edge_label = key\n",
    "        \n",
    "        for key in child.keys(): # key traversal over the node\n",
    "            sub_child = child.get(key)\n",
    "            type_sub_child = type(sub_child)\n",
    "            \n",
    "            if  type_sub_child in [bool, str, int, float] or sub_child is None:\n",
    "                sub_child_node = Node(sub_child, \"atomic\", tree.node_count + 1)\n",
    "                sub_edge = Edge(key, child_node, sub_child_node, \"str\", tree.edge_count + 1, parent_edge_label + \".\" + str(key))\n",
    "                Node.set_outgoingEdge(child_node, sub_edge)\n",
    "                tree.nodes.append(sub_child_node)\n",
    "                tree.node_count += 1\n",
    "                tree.edges.append(sub_edge)\n",
    "                tree.edge_count += 1\n",
    "                \n",
    "            else: # it is of type list or dictionary\n",
    "                sub_child_node = Node(sub_child, \"complex\", tree.node_count + 1)\n",
    "                sub_edge = Edge(key, child_node, sub_child_node, \"str\", tree.edge_count + 1, parent_edge_label + \".\" + str(key))\n",
    "                Node.set_outgoingEdge(child_node, sub_edge)\n",
    "                tree.nodes.append(sub_child_node)\n",
    "                tree.node_count += 1\n",
    "                tree.edges.append(sub_edge)\n",
    "                tree.edge_count += 1\n",
    "                \n",
    "                if type_sub_child == dict: \n",
    "                    # key traversal\n",
    "                    for subkey in sub_child.keys():\n",
    "                        addition(tree, sub_child_node, sub_child.get(subkey), (parent_edge_label + \".\" + str(key) + \".\" + str(subkey)))\n",
    "                else: # it is of type list\n",
    "                    # index traversal\n",
    "                    for j in range(0, len(sub_child)):\n",
    "                        addition(tree, sub_child_node, sub_child[j], parent_edge_label + \".\" + str(key))# + \".\" + str(j+1))\n",
    "    \n",
    "    elif type_child in [str, bool, int, float]: # node is of atomic type\n",
    "        child_node = Node(child, \"atomic\", (tree.node_count + 1))\n",
    "        if type(key) == str:\n",
    "            edge = Edge(key, parent, child_node, \"str\", tree.edge_count + 1, key)\n",
    "        else:\n",
    "            edge = Edge(key, parent, child_node, \"int\", tree.edge_count + 1, key)\n",
    "        Node.set_outgoingEdge(parent, edge)\n",
    "        tree.nodes.append(child_node)\n",
    "        tree.node_count += 1\n",
    "        tree.edges.append(edge)\n",
    "        tree.edge_count += 1\n",
    "                    \n",
    "\n",
    "def json_datamodel(C, c_name): \n",
    "    \"\"\" This functions converts a collection of JSON documents into a collection of JSON  trees\"\"\"\n",
    "    collection_of_trees = []\n",
    "    \n",
    "    for i in range(0, len(C)): # iterate over JSON documents\n",
    "        root_node = Node(\"root\", \"complex\", 1) # create root node\n",
    "        tree = Tree(root_node) # initialize tree object\n",
    "    \n",
    "        document = C[i]\n",
    "\n",
    "        for key in document.keys(): # iteratore over the (first-level) keys in JSON document\n",
    "            addition(tree, root_node, document.get(key), c_name +\".\"+ str(key))\n",
    "        \n",
    "        collection_of_trees.append(tree.tree_in_document())\n",
    "    \n",
    "    return collection_of_trees\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algebraic Operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction operators "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Node**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_node(tree, node_type, node_value=None):  \n",
    "    \"\"\" This function adds a new node to an existing tree (Note that only node of atomic values are possible in this current implementation, can easily be extended)\"\"\"\n",
    "    num_nodes = len(tree.get('nodes'))\n",
    "    if node_value == None:\n",
    "        node_value = num_nodes + 1\n",
    "    \n",
    "    existing_nodes = tree.get('nodes')\n",
    "    existing_nodes[num_nodes + 1] = {'value': node_value, 'outgoingEdge': [], 'type': node_type}\n",
    "    \n",
    "    tree['nodes'] = existing_nodes\n",
    "    \n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edge**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_edge(tree, parent, child, edge_type, label=None,):    \n",
    "    \"\"\" This function adds a new edge to an existing tree\"\"\"\n",
    "    edge_identifier = len(tree.get('edges')) + 1\n",
    "    if label == None: # label is not specified, hence it should be of type int\n",
    "        present_label = edge_identifier\n",
    "    else: \n",
    "        present_label = label\n",
    "        \n",
    "    existing_edges = tree.get('edges')\n",
    "    existing_edges[str(edge_identifier+1)] = {'label': present_label, 'parent': parent, 'child': child, 'type': edge_type}\n",
    "    tree[\"edges\"] = existing_edges\n",
    "    \n",
    "    # update parents outgoingedge identifiers\n",
    "    existing_nodes = tree.get('nodes')\n",
    "    node_info = existing_nodes.get(parent)\n",
    "    existing_nodes[str(parent)] = {'value': node_info.get('value'), 'outgoingEdge': node_info.get('outgoingEdge').append(edge_identifier), 'type': node_info.get('type')}\n",
    "    tree[\"nodes\"] = existing_nodes\n",
    "    \n",
    "    # update labels with childnode identifier\n",
    "    existing_labels = tree.get('labels')\n",
    "    if present_label in existing_labels.keys():\n",
    "        existing_labels[present_label] = existing_labels.get(present_label).append(child)\n",
    "    else:\n",
    "        existing_labels[present_label] = [child]\n",
    "    tree[\"labels\"] = existing_labels    \n",
    "    \n",
    "    return tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction Operators "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Projection operators**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projection(C, labels):\n",
    "    \"\"\" This function only projects the specified labels for each JSON document from the collection\"\"\"\n",
    "    output = []\n",
    "    \n",
    "    for model in C:\n",
    "        result_nodes = {}\n",
    "        result_edges = {}\n",
    "        result_labels = {}\n",
    "        \n",
    "        relevant_nodes = []\n",
    "        labels_requested_not_present = []\n",
    "        \n",
    "        existing_edges = model.get('edges')\n",
    "\n",
    "        # finding relevant nodes\n",
    "        for edge_ident in existing_edges.keys():\n",
    "            edge_info = existing_edges.get(edge_ident)\n",
    "            edge_label = edge_info.get('path')\n",
    "\n",
    "            if edge_label in labels:\n",
    "                result_edges[edge_ident] = edge_info\n",
    "                relevant_nodes = relevant_nodes + [edge_info.get('child')]\n",
    "        \n",
    "\n",
    "        existing_nodes = model.get('nodes')\n",
    "        # for each relevant add its descendants (edges as well as nodes to the result)\n",
    "        while relevant_nodes != []:\n",
    "            for node in relevant_nodes:\n",
    "                node_information = existing_nodes.get(node)\n",
    "                result_nodes[node] = node_information\n",
    "\n",
    "                for out_edge in node_information.get('outgoingEdge'):\n",
    "                    out_edge_info = existing_edges.get(out_edge)\n",
    "                    result_edges[out_edge] = out_edge_info \n",
    "                    child = out_edge_info.get('child')\n",
    "                    if child is not None:\n",
    "                        relevant_nodes = relevant_nodes + [child]\n",
    "                relevant_nodes = relevant_nodes[1:]\n",
    "        \n",
    "        if len(result_nodes) == 0: # no relevant nodes\n",
    "            continue\n",
    "        \n",
    "        existing_labels = model.get(\"labels\")\n",
    "        for label in labels:\n",
    "            result_labels[label] = existing_labels.get(label)\n",
    "        \n",
    "        output.append({\n",
    "                        \"nodes\" : result_nodes,\n",
    "                        \"edges\" : result_edges,\n",
    "                        \"labels\": result_labels\n",
    "                        }) \n",
    "    return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selection operator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selection(C, conditions):  \n",
    "    \"\"\" This function performs one or more selections on each document from the collection\"\"\"\n",
    "    \n",
    "    def regex_condition(cd):\n",
    "        \"\"\" This function reads the condition(s) and transforms it to machine readable instruction\"\"\"\n",
    "        desired_operator = re.findall(r'>=|<=|!=|=|<|>', cd)[0]\n",
    "        variable_names = re.split(r'>=|<=|!=|=|<|>', cd)\n",
    "        \n",
    "        if desired_operator == \"=\": # python syntax\n",
    "            desired_operator = \"==\"\n",
    "            \n",
    "        condition_part1 = re.split(r'\\$', variable_names[0])[-1].strip(\"'\").strip('\"').rstrip().lstrip().strip(\"`\").strip(\"'\").replace(\"'\", \"\")\n",
    "        temp_varia_1 = variable_names[1].lstrip().rstrip()\n",
    "        temp_co_2 = re.split(r'\\$', temp_varia_1)\n",
    "        condition_part2 = temp_co_2[-1].strip(\"'\").strip('\"').rstrip().lstrip().strip(\"`\").strip(\"'\").strip(\"`\").strip(\"'\").replace(\"'\", \"\")\n",
    "\n",
    "        if \"$\" not in temp_varia_1: # comparison with a value \n",
    "            if condition_part2.isnumeric():\n",
    "                if \".\" not in condition_part2 and not \",\" in condition_part2:\n",
    "                    rewritten_condition = \"vertex.get('nodes').get(vertex.get('labels').get('{0}')[0]).get('value') \".format(condition_part1) + \" \" + str(desired_operator) + \" int({0}) \".format(condition_part2)\n",
    "                else:\n",
    "                    rewritten_condition = \"vertex.get('nodes').get(vertex.get('labels').get('{0}')[0]).get('value') \".format(condition_part1) + \" \" + str(desired_operator) + \" float({0}) \".format(condition_part2)\n",
    "            else:\n",
    "                if desired_operator == \"!=\": # switch condition parts\n",
    "                    search_value = \" '{0}' \".format(condition_part2)\n",
    "                    rewritten_condition = \"vertex.get('nodes').get(vertex.get('labels').get('{0}')[0]).get('value') \".format(condition_part1) + \" \" + str(desired_operator) + \" {0} \".format(search_value)     \n",
    "                    rewritten_condition = \"(\"+rewritten_condition + \" or \" + \"len(set([str(vertex.get('nodes').get(item).get('value')) for item in vertex.get('labels').get('{0}')]).intersection(set([{1}]))) == 0)\".format(condition_part1, search_value)\n",
    "                else:\n",
    "                    search_value = \" '{0}' \".format(condition_part2)\n",
    "                    rewritten_condition = \"vertex.get('nodes').get(vertex.get('labels').get('{0}')[0]).get('value') \".format(condition_part1) + \" \" + str(desired_operator) + \" {0} \".format(search_value)     \n",
    "                    rewritten_condition = \"((\"+rewritten_condition + \") or \" + \"(len(set([str(vertex.get('nodes').get(item).get('value')) for item in vertex.get('labels').get('{0}')]).intersection(set([{1}]))) != 0))\".format(condition_part1, search_value)\n",
    "\n",
    "        else: # comparison to JSON doc value\n",
    "            if condition_part1 == condition_part2:\n",
    "                rewritten_condition = \"len([vertex.get('nodes').get(item).get('value') for item in vertex.get('labels').get('{0}')]) != len(set([vertex.get('nodes').get(item).get('value') for item in vertex.get('labels').get('{0}')]))\".format(condition_part1, condition_part2)\n",
    "            else:\n",
    "                rewritten_condition = \"vertex.get('nodes').get(vertex.get('labels').get('{0}')[0]).get('value') \".format(condition_part1) + \" \" + str(desired_operator) + \"vertex.get('nodes').get(vertex.get('labels').get('{0}')[0]).get('value') \".format(condition_part2)     \n",
    "                rewritten_condition = \"(\"+rewritten_condition + \" or \"+\"len(set([vertex.get('nodes').get(item).get('value') for item in vertex.get('labels').get('{0}')]).intersection(set([vertex.get('nodes').get(item).get('value') for item in vertex.get('labels').get('{1}')]))) != 0)\".format(condition_part1, condition_part2)\n",
    "\n",
    "        return rewritten_condition\n",
    "        \n",
    "    split_conditions = []\n",
    "    variable_names = []\n",
    "    readable_condition = \"\"\n",
    "    \n",
    "    # Retrieve the conditions and logical operators\n",
    "    try: \n",
    "        logical_operators = re.findall(' and not | or not | and | or | not ', conditions) + [\" \"]\n",
    "    except TypeError:\n",
    "        logical_operators = [\" \"]\n",
    "\n",
    "    conditions = re.split(' and not | or not | and | or | not ', conditions)\n",
    "    \n",
    "    # Write or conditions to machine executable syntax\n",
    "    or_status = False\n",
    "    or_counter = 0\n",
    "    for i in range(0, len(conditions)):\n",
    "        condition = conditions[i]\n",
    "        translated_condition = regex_condition(condition)\n",
    "        l_o = logical_operators[i]\n",
    "        if l_o in [\"or\", \"or not\", \" or \", \" or not \", \"or \", \"or not\"]:\n",
    "            readable_condition = readable_condition + \"(\"+translated_condition + logical_operators[i]  \n",
    "            or_status = True\n",
    "            or_counter += 1\n",
    "        else:\n",
    "            if l_o == \" \":\n",
    "                if or_status is True:\n",
    "                    if or_counter != 1:\n",
    "                        readable_condition = readable_condition + translated_condition+ \"))\"\n",
    "                    else:\n",
    "                        readable_condition = readable_condition + translated_condition+ \")\"\n",
    "                    or_status = False\n",
    "                    or_counter = 0\n",
    "                else:\n",
    "                    readable_condition = readable_condition + translated_condition \n",
    "            else:\n",
    "                if or_status is True:\n",
    "                    readable_condition = readable_condition + translated_condition+ \")\" + logical_operators[i]  \n",
    "                    or_status = False\n",
    "                    or_counter = 0\n",
    "                else:\n",
    "                    readable_condition = readable_condition + translated_condition + logical_operators[i] \n",
    "            \n",
    "    # input collection\n",
    "    if type(C[0]) == str:\n",
    "        C = C[0]\n",
    "        C = eval(C)\n",
    "\n",
    "    output = []\n",
    "    # perform selection\n",
    "    for vertex in C:\n",
    "        try:\n",
    "            if eval(readable_condition, {\"vertex\": vertex}) is True:\n",
    "                output.append(vertex)\n",
    "        except: TypeError\n",
    "            #print(\"TypeError selection\")\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unorder operator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unorder(C):\n",
    "    \"\"\" This function makes an ordered collection unordered\"\"\"\n",
    "    random.seed(123)\n",
    "    random.shuffle(C)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distinct**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinct(C, unique_identifier): \n",
    "    \"\"\" This function removes all duplicates from a collection based on some unique identifier key\"\"\"\n",
    "    found_identifiers = [None]\n",
    "    distinct_collection = []\n",
    "    \n",
    "    for tree in C:\n",
    "        temp_identifier = None\n",
    "        \n",
    "        try:\n",
    "            temp_identifier = tree.get(\"nodes\").get(tree.get(\"labels\").get(unique_identifier)[0]).get('value')\n",
    "        except:\n",
    "            None\n",
    "        \n",
    "        if temp_identifier not in found_identifiers:\n",
    "            found_identifiers.append(temp_identifier)\n",
    "            distinct_collection.append(tree)\n",
    "            \n",
    "        \n",
    "    return distinct_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sort**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort(col, labels): \n",
    "    \"\"\" This function sorts a collection based on some labels, where the first label determines the first round of ordering\"\"\"\n",
    "    labels.reverse()\n",
    "    \n",
    "    for label in labels:\n",
    "        if \"descending\" in label: # collection should be sorted descendingly based on the label\n",
    "            variable_name = label.strip(\"descending\").replace(\"$\", \"\").lstrip().rstrip().replace(\"'\", \"\")\n",
    "            col.sort(key = lambda x: x.get(\"nodes\").get(x.get(\"labels\").get(variable_name)[0])['value'], reverse=True)\n",
    "        else: # collection should be sorted ascendingly based on the label\n",
    "            variable_name = label.strip(\"ascending\").replace(\"$\", \"\").lstrip().rstrip().replace(\"'\", \"\")\n",
    "            col.sort(key = lambda x: x.get(\"nodes\").get(x.get(\"labels\").get(variable_name)[0])['value'], reverse=False)\n",
    "    \n",
    "    return col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Join** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_value(doc_in_tree, name):\n",
    "    \"\"\" This function retrieve a specfic node value, this function is utilized in the join function \"\"\"\n",
    "    identifier = doc_in_tree.get(\"labels\").get(name.strip(\"'\"))\n",
    "    \n",
    "    try: \n",
    "        if len(identifier) != 1:\n",
    "            result = []\n",
    "            for temp_iden in identifier:\n",
    "                result.append(doc_in_tree.get(\"nodes\").get(temp_iden).get(\"value\"))\n",
    "            return set(result) # als het goed is is deze unordered\n",
    "        else:\n",
    "            return doc_in_tree.get(\"nodes\").get(identifier[0]).get(\"value\")\n",
    "    except None or TypeError:\n",
    "        #print(\"AN ERROR OCCURED, NODE COULD NOT BE FOUND!\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join(collection_names, conditions):\n",
    "    \"\"\" This function joins two collections based on some conditions\"\"\"\n",
    "    # input collections \n",
    "    if type(collection_names[0]) == str:\n",
    "        c1 = eval(collection_names[0])\n",
    "    else:\n",
    "        c1 = collection_names[0]\n",
    "        \n",
    "    if type(collection_names[1]) == str:\n",
    "        c2 = eval(collection_names[1])\n",
    "    else:\n",
    "        c2 = collection_names[1]\n",
    "    \n",
    "    # transform condition(s) if provided\n",
    "    condition = conditions \n",
    "    desired_operator = re.findall(r'>=|=<|=|<|>', condition)[0]\n",
    "    variable_names = re.split(r'>=|=<|=|<|>', condition)\n",
    "    \n",
    "    if collection_names[0] in variable_names[0]:\n",
    "        condition_part1 = re.split(r'\\$', variable_names[0])[-1].strip(\"'\").strip('\"').rstrip().lstrip().strip(\"`\").strip(\"'\")\n",
    "        condition_part2 = re.split(r'\\$', variable_names[1])[-1].strip(\"'\").strip('\"').rstrip().lstrip().strip(\"`\").strip(\"'\")\n",
    "    else:\n",
    "        condition_part2 = re.split(r'\\$', variable_names[0])[-1].strip(\"'\").strip('\"').rstrip().lstrip().strip(\"`\").strip(\"'\")\n",
    "        condition_part1 = re.split(r'\\$', variable_names[1])[-1].strip(\"'\").strip('\"').rstrip().lstrip().strip(\"`\").strip(\"'\")\n",
    "    \n",
    "    join_result = []\n",
    "\n",
    "    for c1_doc in c1:\n",
    "        retrieved_value_c1 = retrieve_value(c1_doc, condition_part1) \n",
    "        \n",
    "        for c2_doc in c2:\n",
    "            retrieved_value_c2 = retrieve_value(c2_doc, condition_part2)\n",
    "            d_o = desired_operator.strip(\" \")\n",
    "            if d_o == \"=\": # python syntax \n",
    "                if retrieved_value_c1 == retrieved_value_c2: \n",
    "                    # add the join into the new result\n",
    "                    # it now basically is a cartesian prodcut of tow singleton collections\n",
    "                    join_result.append(cartesian_product([[c1_doc], [c2_doc]])[0])\n",
    "            else:\n",
    "                if eval(retrieved_value_c1 +\" \" + d_o + \" \" + retrieved_value_c2):\n",
    "                    join_result.append(cartesian_product([[c1_doc], [c2_doc]])[0])\n",
    "                \n",
    "    return join_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cartesian Product**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartesian_product(C): \n",
    "    \"\"\" This function performs a cartesian product over de collections provided in C\"\"\"\n",
    "    # input collections\n",
    "    if type(C[0]) == str:\n",
    "        C1 = eval(C[0])\n",
    "    else:\n",
    "        C1 = C[0]\n",
    "    if type(C[1]) ==  str:\n",
    "        C2 = eval(C[1])\n",
    "    else:\n",
    "        C2 = C[1]\n",
    "    \n",
    "    cartesian_result = []\n",
    "\n",
    "    for tree_from_c1 in C1:\n",
    "        current_tree_from_c1 = tree_from_c1.copy()  #always .copy() when iterating over dictionaries\n",
    "        num_nodes = len(current_tree_from_c1.get('nodes').keys())\n",
    "        num_edges = len(current_tree_from_c1.get('edges').keys())\n",
    "        \n",
    "        nodes_tree1 = current_tree_from_c1.get(\"nodes\")\n",
    "        edges_tree1 = current_tree_from_c1.get(\"edges\")\n",
    "        labels_tree1 = current_tree_from_c1.get(\"labels\")\n",
    "        \n",
    "        # add the second tree to the cartesian product, while updating the identifiers for the nodes and edges (+ their referals)\n",
    "        for tree_from_c2 in C2:\n",
    "            current_tree_from_c2 = tree_from_c2.copy()\n",
    "            \n",
    "            current_nodes = nodes_tree1.copy()\n",
    "            current_edges = edges_tree1.copy()\n",
    "            current_labels = labels_tree1.copy()\n",
    "            \n",
    "            for node in list(current_tree_from_c2.get(\"nodes\").keys()):\n",
    "                specific_node_t2 = current_tree_from_c2.get(\"nodes\").get(node).copy()\n",
    "                \n",
    "                if specific_node_t2.get('type') == 'complex':\n",
    "                    specific_node_t2['value'] = specific_node_t2['value'] + num_nodes\n",
    "                \n",
    "                specific_node_t2[\"outgoingEdge\"] = [e+num_edges for e in specific_node_t2[\"outgoingEdge\"]]\n",
    "    \n",
    "                current_nodes[node+num_nodes] = specific_node_t2\n",
    "            \n",
    "            for edge in list(current_tree_from_c2.get(\"edges\").keys()):\n",
    "                specific_edge_t2 = current_tree_from_c2.get(\"edges\").get(edge).copy()\n",
    "                \n",
    "                specific_edge_t2['parent'] = int(specific_edge_t2[\"parent\"]) + num_nodes\n",
    "                specific_edge_t2['child'] = int(specific_edge_t2[\"child\"]) + num_nodes\n",
    "                    \n",
    "                current_edges[edge+num_edges] = specific_edge_t2\n",
    "            \n",
    "            for l in list(current_tree_from_c2.get(\"labels\").keys()):\n",
    "                \n",
    "                temp_list = []\n",
    "                for child_reference in current_tree_from_c2.get(\"labels\").get(l):\n",
    "                    temp_list.append(child_reference+num_nodes)\n",
    "\n",
    "                if l in list(current_labels.keys()):\n",
    "                    current_labels[l] = current_labels.get(l) + temp_list\n",
    "                else:\n",
    "                    current_labels[l] = temp_list\n",
    "                \n",
    "            tree_product = {\n",
    "                'nodes': current_nodes,\n",
    "                'edges': current_edges,\n",
    "                'labels': current_labels\n",
    "            }\n",
    "            \n",
    "            cartesian_result.append(tree_product)\n",
    "            \n",
    "    return cartesian_result\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Union**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def union(C1, C2):\n",
    "    \"\"\" This function adds two collections together \"\"\"\n",
    "    result = C1 + C2\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intersection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(C1, C2, unique_identifier):\n",
    "    \"\"\" This function returns the JSON documents that are present in both collection, note that in order to compare if two JSON documents are identical a unique identifer key should be specified\"\"\"\n",
    "    found_identifiers = [None]\n",
    "    result = []\n",
    "    \n",
    "    for tree in C2:\n",
    "        temp_identifier = None\n",
    "        \n",
    "        try:\n",
    "            temp_identifier = tree.get(\"nodes\").get(tree.get(\"labels\").get(unique_identifier)[0]).get('value')\n",
    "        except:\n",
    "            None\n",
    "        \n",
    "        if temp_identifier is not None:\n",
    "            found_identifiers.append(temp_identifier)\n",
    "    \n",
    "    for tree in C1:\n",
    "        temp_identifier = None\n",
    "        \n",
    "        try:\n",
    "            temp_identifier = tree.get(\"nodes\").get(tree.get(\"labels\").get(unique_identifier)[0]).get('value')\n",
    "        except:\n",
    "            None\n",
    "        \n",
    "        if temp_identifier is not None and temp_identifier in found_identifiers: # an intersecting document has been found\n",
    "            result.append(tree)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Difference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference(C1, C2, unique_identifier):\n",
    "    \"\"\" This function returns all JSON documents that are present in the first collection but not in the second collection (agian note the unique identifier)\"\"\"\n",
    "    found_identifiers = [None]\n",
    "    result = []\n",
    "    \n",
    "    for tree in C2:\n",
    "        temp_identifier = None\n",
    "        \n",
    "        try:\n",
    "            temp_identifier = tree.get(\"nodes\").get(tree.get(\"labels\").get(unique_identifier)[0]).get('value')\n",
    "        except:\n",
    "            None\n",
    "        \n",
    "        if temp_identifier is not None:\n",
    "            found_identifiers.append(temp_identifier)\n",
    "    \n",
    "    for tree in C1:\n",
    "        temp_identifier = None\n",
    "        \n",
    "        try:\n",
    "            temp_identifier = tree.get(\"nodes\").get(tree.get(\"labels\").get(unique_identifier)[0]).get('value')\n",
    "        except:\n",
    "            None\n",
    "        \n",
    "        if temp_identifier is not None and temp_identifier not in found_identifiers: # a difference has been found\n",
    "            result.append(tree)\n",
    "    \n",
    "    return result\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifiying the algebraic operators done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSONiq "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSONiq utilizes the engine RUMBLE (source: https://colab.research.google.com/github/RumbleDB/rumble/blob/master/RumbleSandbox.ipynb#scrollTo=oKidcQCmLsS5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from IPython.core.magic import register_line_cell_magic\n",
    "\n",
    "@register_line_cell_magic\n",
    "def rumble(line, cell=None):\n",
    "    if cell is None:\n",
    "        data = line\n",
    "    else:\n",
    "        data = cell\n",
    "\n",
    "    start = time.time()                                                         \n",
    "    response = json.loads(requests.post(server, data=data).text)                   \n",
    "    end = time.time()                                                              \n",
    "    print(\"Took: %s ms\" % (end - start))\n",
    "\n",
    "    if 'warning' in response:\n",
    "        print(json.dumps(response['warning']))\n",
    "    if 'values' in response:\n",
    "        for e in response['values']:\n",
    "            print(json.dumps(e))\n",
    "    elif 'error-message' in response:\n",
    "        return response['error-message']\n",
    "    else:\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we run jsoniq on a public server for convenience\n",
    "server ='http://public.rumbledb.org:9090/jsoniq'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  JSONiq query to JSON Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lexer + Parser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on : https://gist.github.com/eliben/5797351\n",
    "class Token(object): \n",
    "    \"\"\" A simple Token structure. Contains the token type, value and position. \"\"\"\n",
    "    def __init__(self, type, val, pos):\n",
    "        self.type = type\n",
    "        self.val = val\n",
    "        self.pos = pos\n",
    "\n",
    "    def __str__(self):\n",
    "        return '%s(%s) at %s' % (self.type, self.val, self.pos)\n",
    "\n",
    "\n",
    "class LexerError(Exception):\n",
    "    \"\"\" Throw lexer error \"\"\"\n",
    "    def __init__(self, pos):\n",
    "        self.pos = pos\n",
    "\n",
    "\n",
    "class Lexer(object):\n",
    "    \"\"\" A regex-based lexer/tokenizer \"\"\"\n",
    "    def __init__(self, rules, skip_whitespace=True):\n",
    "        \"\"\" initialize a lexer \"\"\" \n",
    "        idx = 1\n",
    "        regex_parts = []\n",
    "        self.group_type = {}\n",
    "\n",
    "        for regex, type in rules:\n",
    "            groupname = 'GROUP%s' % idx\n",
    "            regex_parts.append('(?P<%s>%s)' % (groupname, regex))\n",
    "            self.group_type[groupname] = type\n",
    "            idx += 1\n",
    "\n",
    "        self.regex = re.compile('|'.join(regex_parts))\n",
    "        self.skip_whitespace = skip_whitespace\n",
    "        self.re_ws_skip = re.compile('\\S')\n",
    "\n",
    "    def input(self, q):\n",
    "        \"\"\" Initialize the lexer \"\"\"\n",
    "        self.query = q\n",
    "        self.pos = 0\n",
    "\n",
    "    def token(self):\n",
    "        \"\"\" Return the next token (a Token object) found in the input \"\"\"\n",
    "        if self.pos >= len(self.query):\n",
    "            return None\n",
    "        else:\n",
    "            if self.skip_whitespace:\n",
    "                m = self.re_ws_skip.search(self.query, self.pos)\n",
    "\n",
    "                if m:\n",
    "                    self.pos = m.start()\n",
    "                else:\n",
    "                    return None\n",
    "\n",
    "            m = self.regex.match(self.query, self.pos)\n",
    "            if m:\n",
    "                groupname = m.lastgroup\n",
    "                tok_type = self.group_type[groupname]\n",
    "                tok = Token(tok_type, m.group(groupname), self.pos)\n",
    "                self.pos = m.end()\n",
    "                return tok\n",
    "\n",
    "            # if here, no rule was matched\n",
    "            raise LexerError(self.pos)\n",
    "\n",
    "    def tokens(self):\n",
    "        \"\"\" Returns an iterator to the tokens found in the input \"\"\"\n",
    "        while 1:\n",
    "            tok = self.token()\n",
    "            if tok is None: break\n",
    "            yield tok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_lexer():\n",
    "    \"\"\" This function initalizes the required lexer\"\"\"\n",
    "    \n",
    "    # Necessary lexer rules for the translation to JAL\n",
    "    rules = [\n",
    "            ('let \\$.+(:=)','VARIABELE ASSIGNMENT'),\n",
    "            \n",
    "            (\"for \\$.+? in .+?(?=for|where|return|for|let|order by)\", 'PROJECTION'),\n",
    "            (\"where \\$.*?(?=for|return|for|let|order by|where)\", 'CONDITION'),\n",
    "            \n",
    "        \n",
    "            ('return .+?(?=return)', 'RETURN'),\n",
    "            ('return .+', 'RETURN'),\n",
    "        \n",
    "            ('order by .*?(?=return|for|let|order by)', \"SORT\")\n",
    "        ]\n",
    "\n",
    "    lx = Lexer(rules, skip_whitespace=True)\n",
    "    return lx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create the lexer\n",
    "lx = initialize_lexer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_lexer_part1(q): \n",
    "    \"\"\" This function parses the query to a basic syntax, that later (part2) will be used for the tranlation to actual JAL\"\"\"\n",
    "    \n",
    "    # Lexer\n",
    "    lx.input(q)\n",
    "    lex_result = []\n",
    "    try:\n",
    "        for match in lx.tokens():\n",
    "            lex_result.append(str(match).split(r\" at \")[0])\n",
    "    except LexerError as err:\n",
    "        print('LexerError at position %s' % err.pos)\n",
    "    \n",
    "    # tranlate the lexer result to a query process\n",
    "    query_process = []\n",
    "    function_call = []\n",
    "    while lex_result != []:\n",
    "        action = lex_result[0]\n",
    "        \n",
    "\n",
    "        if len(lex_result) >1:\n",
    "            \n",
    "            if function_call == []:\n",
    "                # start of a new clause, meaning that it has to begin with either let or for\n",
    "                if \"VARIABELE ASSIGNMENT\" in action: # skip \n",
    "                    lex_result = lex_result[1:]\n",
    "\n",
    "                if \"PROJECTION\" in action:\n",
    "                    function_call.append(action)\n",
    "                    lex_result = lex_result[1:]\n",
    "                    \n",
    "            else: \n",
    "                if \"PROJECTION\" in action or \"CONDITION\" in action:\n",
    "                    function_call.append(action)\n",
    "                    lex_result = lex_result[1:]\n",
    "            \n",
    "                if \"VARIABELE ASSIGNMENT\" in action: # skip \n",
    "                    lex_result = lex_result[1:]\n",
    "                \n",
    "                if \"SORT\" in action:\n",
    "                    function_call.append(action)\n",
    "                    lex_result = lex_result[1:]\n",
    "\n",
    "                if \"RETURN\" in action:\n",
    "                    function_call.append(action)\n",
    "                    query_process.append(function_call)\n",
    "                    function_call = []\n",
    "                    lex_result = lex_result[1:]\n",
    "            \n",
    "        else:\n",
    "            # has to be a return statement\n",
    "            if not \"RETURN\" in action:\n",
    "                return \"Error, the query does not end with a return clause!\"\n",
    "            else:\n",
    "                # Assume that it is the return clause belonging to the let clause (so does not have to be added to query process)\n",
    "                lex_result = []\n",
    "    \n",
    "    return query_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algebra_input(converted_query): # second part for the parsing\n",
    "    \"\"\" This function tranlates the query process result from parser part 1 (previous function) to JAL\"\"\"\n",
    "    \n",
    "    operator_calls = []\n",
    "    \n",
    "    for clause in converted_query:\n",
    "        call = []\n",
    "        condition_clause_temp_variables = []\n",
    "        collection_names = []\n",
    "        conditions_total = \"\"\n",
    "        for action in clause:\n",
    "            if \"PROJECTION\" in action:\n",
    "                # find collection name(s)\n",
    "                temp_collection_names = re.findall(r\"\\$([a-zA-Z]+)\\[\\]\", action)\n",
    "                collection_names = collection_names + temp_collection_names\n",
    "                \n",
    "                temp_names = re.findall(r\"\\$([a-zA-Z]+) in\", action)\n",
    "                for i in range(0, len(temp_names)):\n",
    "                    condition_clause_temp_variables.append([temp_names[i], temp_collection_names[i]])\n",
    "                \n",
    "                \n",
    "            if \"CONDITION\" in action:\n",
    "                conditions = action.strip(\"CONDITION(where\").strip(\")\").lstrip().rstrip().strip(\"'\").replace(\")\", \"\").replace(\"jn:members(\", \"\")\n",
    "\n",
    "                for temp_key in condition_clause_temp_variables:\n",
    "                    conditions = conditions.replace(r\"${0}.\".format(temp_key[0]), \"${0}.\".format(temp_key[1]))\n",
    "                \n",
    "                if len(conditions_total) == 0:\n",
    "                    conditions_total = conditions\n",
    "                else:\n",
    "                    conditions_total = conditions_total + \" and \" + conditions\n",
    "            \n",
    "            if \"SORT\" in action:\n",
    "                if len(call) == 0: # order by clause over a stand-alone projection\n",
    "                    if len(collection_names) == 1:\n",
    "                        if len(conditions_total) != 0:\n",
    "                            call.append(\"selection\")\n",
    "                            call.append(collection_names)\n",
    "                            call.append(conditions_total)\n",
    "                        else:\n",
    "                            call.append(\"projection\")\n",
    "                            call.append(collection_names)\n",
    "                    else:\n",
    "                        call.append(\"join\") \n",
    "                        call.append(collection_names)\n",
    "                        call.append(conditions_total)\n",
    "                        \n",
    "                \n",
    "                sort_labels = action.strip(\"SORT(order by\").strip(\")\").lstrip().rstrip().split(\",\")\n",
    "                \n",
    "                for temp_key in condition_clause_temp_variables:\n",
    "                    for k in range(0, len(sort_labels)):\n",
    "                        sort_labels[k] = sort_labels[k].replace(r\"${0}.\".format(temp_key[0]), r\"${0}.\".format(temp_key[1]))\n",
    "                    \n",
    "                call.append(\"sort\")\n",
    "                call.append(sort_labels)\n",
    "            \n",
    "            \n",
    "            if \"RETURN\" in action:\n",
    "                \n",
    "                if call == []:\n",
    "                    if len(collection_names) == 1:\n",
    "                        if conditions_total != \"\":\n",
    "                            call.append(\"selection\")\n",
    "                            call.append(collection_names)\n",
    "                            call.append(conditions_total)\n",
    "                    else:\n",
    "                        call.append(\"join\") \n",
    "                        call.append(collection_names)\n",
    "                        call.append(conditions_total)\n",
    "                \n",
    "                if bool(re.match(r'RETURN\\(return \\{.+\\}.+', action)) is True: # white spaces are a bottleneck here\n",
    "                    labels = re.split(\",\", action)\n",
    "                    for temp_key in condition_clause_temp_variables:\n",
    "                        for j in range(0, len(labels)):\n",
    "                            temp_label_j = labels[j].replace(\"{0}.\".format(temp_key[0]), \"{0}.\".format(temp_key[1])).replace(\"'\", \"\").replace('\"', \"\").replace(\")\", \"\").replace(\"}\", \"\").replace(\"$\", \"\").rstrip()\n",
    "                            labels[j] = temp_label_j.split(\":\")[-1].lstrip().rstrip()\n",
    "                    call.append(\"projection\")\n",
    "                    call.append(labels)  \n",
    "                \n",
    "                if bool(re.match(r'RETURN\\(return \\$.+', action)) is True: \n",
    "                    # intermediate output is the output   \n",
    "                    pass\n",
    "                    \n",
    "        operator_calls.append(call)        \n",
    "    \n",
    "    return operator_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_to_collection(input_collection_trees):\n",
    "    \"\"\" Convert a collection of trees back to a collection of JSON documents \"\"\"\n",
    "    \n",
    "    output = []\n",
    "    \n",
    "    \n",
    "    def find_value(tree_info, n, e_label, traversed_edge_identifiers):\n",
    "        \"\"\" this function find the value of the node the edge refers to\"\"\"\n",
    "        if n.get(\"type\") == \"atomic\": # node is of atomic type, hence a leaf node and further traversal is not required\n",
    "            node_value = n.get(\"value\")\n",
    "            if len(traversed_edge_identifiers) == 1:\n",
    "                e_label = e_label.split(\".\")[-1]\n",
    "                return {e_label: node_value}, None\n",
    "            else: \n",
    "                return node_value, None\n",
    "        \n",
    "        else: # node is complex, furter traversal is required\n",
    "            type_n_edge = tree_info.get(\"edges\").get(n.get(\"outgoingEdge\")[0]).get(\"type\")\n",
    "                        \n",
    "            if type_n_edge == \"int\": # then the complex type refers to a list\n",
    "                temp_list = []\n",
    "                for e in n.get(\"outgoingEdge\"):\n",
    "                    e_info = tree_info.get(\"edges\").get(e)\n",
    "                    traversed_edge_identifiers.append(e)\n",
    "                    temp_list.append(find_value(tree_info, tree_info.get(\"nodes\").get(e_info.get(\"child\")), e_info.get(\"label\"), traversed_edge_identifiers)[0])\n",
    "                e_label = e_label.split(\".\")[-1]\n",
    "                return {e_label: temp_list}, traversed_edge_identifiers\n",
    "            else: # complex type refers to a dictionary\n",
    "                temp_dict = {}\n",
    "                for e in n.get(\"outgoingEdge\"):\n",
    "                    try:\n",
    "                        e_info = tree_info.get(\"edges\").get(e)\n",
    "                        traversed_edge_identifiers.append(e)\n",
    "                        temp_dict[e_info.get(\"label\")] = find_value(tree_info, tree_info.get(\"nodes\").get(e_info.get(\"child\")), e_info.get(\"label\"), traversed_edge_identifiers)[0]\n",
    "                    except: IndexError\n",
    "                e_label = e_label.split(\".\")[-1]\n",
    "                return {e_label: temp_dict}, traversed_edge_identifiers\n",
    "                \n",
    "    \n",
    "    # Convert each tree from the collection to a document\n",
    "    for doc in input_collection_trees:\n",
    "        doc_edges = list(doc.get(\"edges\").keys())\n",
    "        doc_result = { }\n",
    "\n",
    "        while doc_edges != []:\n",
    "            e_info = doc.get(\"edges\").get(doc_edges[0])\n",
    "            if e_info.get(\"type\") == \"str\":\n",
    "                node = doc.get(\"nodes\").get(doc.get(\"labels\").get(e_info.get(\"path\"))[0])\n",
    "            else:\n",
    "                node = doc.get(\"nodes\").get(e_info.get(\"child\"))\n",
    "            traversed_identifiers = [doc_edges[0]]\n",
    "            temp_value, tr_id = find_value(doc, node, e_info.get(\"label\"), traversed_identifiers)\n",
    "            doc_result[doc_edges[0]] = temp_value\n",
    "            traversed_identifiers.append(tr_id)\n",
    "            doc_edges = [e for e in doc_edges if e not in traversed_identifiers]\n",
    "\n",
    "        # add document to output\n",
    "        temp_output = { }\n",
    "        for temp_res in doc_result.values():\n",
    "            temp_output.update(temp_res)\n",
    "        output.append(temp_output)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer(unoptimzed_order):\n",
    "    \"\"\" this function applies my optimization strategy to the input query expressed in JAL \"\"\"\n",
    "    \n",
    "    process_order = []\n",
    "    sorted_possibly = []\n",
    "    \n",
    "    for process in unoptimzed_order:\n",
    "        temp_order = []\n",
    "        sub_process = process\n",
    "        \n",
    "        while sub_process != []:\n",
    "            function_name = sub_process[0]\n",
    "\n",
    "            if function_name == \"join\" or function_name == \"selection\":\n",
    "                # decompse the join if possible\n",
    "                # collection sizes\n",
    "                relevant_collections = process[1]\n",
    "                collection_sizes = {}\n",
    "                for collection in relevant_collections:\n",
    "                    collection_sizes[collection] = len(eval(collection))\n",
    "\n",
    "                collection_sizes = sorted(collection_sizes.items(), key=lambda x: x[1])\n",
    "                \n",
    "                # perform most restrictive selections (= conditions) first\n",
    "                conditions = process[2]\n",
    "                and_conditions = re.split(' and ', conditions) \n",
    "                most_restrictive_conditions = []\n",
    "                inbetween_conditions = []\n",
    "                least_favourable_conditons = []\n",
    "                for and_condition in and_conditions:\n",
    "                    logics = re.findall(r'and not|or not| or | and ', and_condition)\n",
    "                    desired_operator = re.findall(r'>=|<=|=|<|>', and_condition)\n",
    "                    variable_names = re.split(r'>=|<=|=|<|>', and_condition)\n",
    "                    \n",
    "                    if len(variable_names) <= 2:\n",
    "                        desired_operator = desired_operator[0]\n",
    "                        if desired_operator == \"=\": \n",
    "                            if not \"$\" in variable_names[1]:\n",
    "                                if \"not $\" in variable_names[0]:\n",
    "                                    most_restrictive_conditions.append(variable_names[0].replace(\"not\", \"\") + \"!=\" + variable_names[1])\n",
    "                                else:\n",
    "                                    most_restrictive_conditions.append(variable_names[0] + desired_operator + variable_names[1])\n",
    "                            else:\n",
    "                                if \"not $\" in variable_names[0]:\n",
    "                                    least_favourable_conditons.append(variable_names[1] + \"!=\" + variable_names[0].replace(\"not\", \"\"))\n",
    "                                else:\n",
    "                                    least_favourable_conditons.append(variable_names[0] + desired_operator + variable_names[1])\n",
    "                        else:\n",
    "                            if not \"$\" in variable_names[1]:\n",
    "                                if \"not $\" in variable_names:\n",
    "                                    inbetween_conditions.append(variable_names[1] + desired_operator + variable_names[0])\n",
    "                                else:\n",
    "                                    inbetween_conditions.append(variable_names[0] + desired_operator + variable_names[1])\n",
    "                            else:\n",
    "                                if \"not $\" in variable_names:\n",
    "                                    least_favourable_conditons.append(variable_names[1] + desired_operator + variable_names[0])\n",
    "                                else:\n",
    "                                    least_favourable_conditons.append(variable_names[0] + desired_operator + variable_names[1])\n",
    "                    else:\n",
    "                        temp_most_restrictive = []\n",
    "                        temp_inbetween_restrictive = []\n",
    "                        temp_least_restrictive = []\n",
    "                        logics = logics + [\" \"]\n",
    "                        temp_operators = desired_operator\n",
    "                        variable_names = re.split(r' and | or ', and_condition)\n",
    "                        temp_total = []\n",
    "                        for temp_vn in variable_names:\n",
    "                            temp_total = temp_total + re.split(r'>=|=<|=|<|>', temp_vn)\n",
    "                        variable_names = temp_total\n",
    "                        for t in range(0, int(len(variable_names)/2)):\n",
    "                            temp_logic = logics[t]\n",
    "                            desired_operator = temp_operators[t]\n",
    "    \n",
    "                            if desired_operator == \"=\": \n",
    "                                if not \"$\" in variable_names[t*2+1]:\n",
    "                                    if \" not $\" in variable_names[2*t]:\n",
    "                                        temp_most_restrictive.append(variable_names[2*t].replace(\"not\", \"\") + \"!=\" + variable_names[t*2+1] + \" \" + temp_logic)\n",
    "                                    else:\n",
    "                                        temp_most_restrictive.append(variable_names[2*t] + desired_operator + variable_names[t*2+1] + \" \" + temp_logic)\n",
    "                                else:\n",
    "                                    if \" not $\" in variable_names[t*2+1]:\n",
    "                                        temp_least_restrictive.append(variable_names[t*2+1] + \"!=\" + variable_names[t].replace(\"not\", \"\") + \" \" + temp_logic)\n",
    "                                    else:\n",
    "                                        temp_least_restrictive.append(variable_names[2*t] + desired_operator + variable_names[t*2+1] + \" \" + temp_logic)\n",
    "                            else:\n",
    "                                if not \"$\" in variable_names[t*2+1]:\n",
    "                                    if \"not $\" in variable_names:\n",
    "                                        temp_inbetween_restrictive.append(variable_names[t*2+1] + desired_operator + variable_names[2*t] + \" \" + temp_logic)\n",
    "                                    else:\n",
    "                                        temp_inbetween_restrictive.append(variable_names[2*t] + desired_operator + variable_names[t*2+1] + \" \" + temp_logic)\n",
    "                                else:\n",
    "                                    if \"not $\" in variable_names:\n",
    "                                        temp_least_restrictive.append(variable_names[t*2+1] + desired_operator + variable_names[2*t] + \" \" + temp_logic)\n",
    "                                    else:\n",
    "                                        temp_least_restrictive.append(variable_names[2*t] + desired_operator + variable_names[t*2+1] + \" \" + temp_logic)\n",
    "                        \n",
    "                        if temp_inbetween_restrictive == [] and temp_least_restrictive == []:\n",
    "                            concantenated_condition = \"\"\n",
    "                            for entry in temp_most_restrictive:\n",
    "                                concantenated_condition = concantenated_condition + \" \" + entry\n",
    "                            most_restrictive_conditions.append(concantenated_condition)\n",
    "                        elif temp_least_restrictive == []:\n",
    "                            concantenated_condition = \"\"\n",
    "                            for entry in temp_most_restrictive:\n",
    "                                concantenated_condition = concantenated_condition + \" \" + entry\n",
    "                            for entry in temp_inbetween_restrictive:\n",
    "                                concantenated_condition = concantenated_condition + \" \" + entry\n",
    "                            inbetween_conditions.append(concantenated_condition)\n",
    "                        else:\n",
    "                            concantenated_condition = \"\"\n",
    "                            for entry in temp_most_restrictive:\n",
    "                                concantenated_condition = concantenated_condition + \" \" + entry\n",
    "                            for entry in temp_inbetween_restrictive:\n",
    "                                concantenated_condition = concantenated_condition + \" \" + entry\n",
    "                            for entry in temp_least_restrictive:\n",
    "                                concantenated_condition = concantenated_condition + \" \" + entry\n",
    "                            least_favourable_conditons.append(concantenated_condition)\n",
    "                            \n",
    "\n",
    "                while most_restrictive_conditions != []:\n",
    "                    for smallest_collection in collection_sizes:\n",
    "                        smallest_collection = smallest_collection[0]\n",
    "                        for most_restrictive_condition in most_restrictive_conditions:\n",
    "                            if smallest_collection in most_restrictive_condition:\n",
    "                                temp_order.append(['selection', [smallest_collection], most_restrictive_condition])\n",
    "                                most_restrictive_conditions.remove(most_restrictive_condition)\n",
    "\n",
    "                while inbetween_conditions != []:\n",
    "                    for smallest_collection in collection_sizes:\n",
    "                        smallest_collection = smallest_collection[0]\n",
    "                        for inbetween_condition in inbetween_conditions:\n",
    "                            if smallest_collection in inbetween_condition:\n",
    "                                temp_order.append(['selection', [smallest_collection], inbetween_condition])\n",
    "                                inbetween_conditions.remove(inbetween_condition)\n",
    "                \n",
    "                ordered_collections = []\n",
    "                str_ordered_collections = \"\"\n",
    "                for colle in collection_sizes:\n",
    "                    ordered_collections.append(colle[0])\n",
    "                    str_ordered_collections = str_ordered_collections + str(colle[0]) + \"|\"\n",
    "                str_ordered_collections = str_ordered_collections[:-1]\n",
    "                \n",
    "                while least_favourable_conditons != []:\n",
    "                    for smallest_collection in ordered_collections:\n",
    "                        join_condition_order = \"\"\n",
    "                        for least_favourable_conditon in least_favourable_conditons:\n",
    "                            if smallest_collection in least_favourable_conditon:\n",
    "                                join_collections = re.findall(r\"{0}\".format(str_ordered_collections), least_favourable_conditon)\n",
    "                                join_condition_order = join_condition_order + least_favourable_conditon\n",
    "                                least_favourable_conditons.remove(least_favourable_conditon)\n",
    "                                temp_order.append(['join', list(set(join_collections)), join_condition_order])\n",
    "                sub_process = sub_process[3:]\n",
    "            \n",
    "            if function_name == \"projection\":\n",
    "                temp_order.append(['projection', sub_process[1]])\n",
    "                sub_process = sub_process[2:]\n",
    "            \n",
    "            if function_name == \"sort\":\n",
    "                sorted_possibly = sorted_possibly + ['sort', sub_process[1]]\n",
    "                sub_process = sub_process[2:]\n",
    "                \n",
    "        process_order.append(temp_order)\n",
    "    process_order[0].append(sorted_possibly)\n",
    "    return process_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execution(query, optimization): \n",
    "    \"\"\" This function executes a query with optimization \"\"\"\n",
    "    start = time.time() \n",
    "    \n",
    "    # Parsing\n",
    "    part1 = parse_lexer_part1(query)\n",
    "    process_order = algebra_input(part1)\n",
    "\n",
    "    if optimization is True:\n",
    "        process_order = optimizer(process_order)[0]\n",
    "    query_result = None\n",
    "\n",
    "    updated_collections = {} # to keep track of intermediate collection results\n",
    "    joined_collections = [] # to keep track of the collections that are already present in a join\n",
    "    \n",
    "    for q in process_order:\n",
    "        temp = q\n",
    "        temp_collection = []\n",
    "        \n",
    "        while temp != []:\n",
    "            function_name = temp[0]\n",
    "\n",
    "            if function_name == 'join':\n",
    "                collection_names = temp[1]\n",
    "                                \n",
    "                if query_result is None:\n",
    "                    first_col = collection_names[0]\n",
    "                    joined_collections.append(first_col)\n",
    "                    if first_col in updated_collections.keys(): # an operation has already been performed on the collection\n",
    "                        first_col = updated_collections.get(first_col)\n",
    "                        \n",
    "                    query_result = first_col\n",
    "                    for i in range(1, len(collection_names)):\n",
    "                        next_col = collection_names[i]\n",
    "                        \n",
    "                        if next_col not in joined_collections:\n",
    "                            joined_collections.append(next_col)\n",
    "                            if next_col in updated_collections.keys(): # an operation has already been performed on the collection\n",
    "                                next_col = updated_collections.get(next_col)\n",
    "        \n",
    "                            query_result = cartesian_product([query_result, next_col])\n",
    "\n",
    "                    \n",
    "                    if query_result == first_col: # technically there is no query result yet\n",
    "                        query_result = None\n",
    "  \n",
    "                else: # an intermediate query result already exists \n",
    "                    for i in range(0, len(collection_names)):\n",
    "                        collection_name_name = collection_names[i]\n",
    "                    \n",
    "                        if collection_name_name in updated_collections.keys(): # an operation has already been performed on the collection\n",
    "                            collection_name = updated_collections.get(collection_name_name)\n",
    "                            \n",
    "                        if collection_name_name not in joined_collections: # collection is not yet present in the join \n",
    "                            if type(collection_name_name) == str: \n",
    "                                collection_name = collection_name_name\n",
    "                            query_result = cartesian_product([query_result, collection_name])\n",
    "                            joined_collections.append(collection_name_name)\n",
    "\n",
    "                if temp[2] != \"\": # a selection should be performed\n",
    "                    query_result = selection(query_result, temp[2])\n",
    "                temp = temp[3:]\n",
    "\n",
    "            elif function_name == \"projection\":\n",
    "                if optimization is True:\n",
    "                    if query_result == None:\n",
    "                        temp_result = updated_collections.get(list(updated_collections.keys())[0])\n",
    "                        query_result = projection(temp_result, temp[1])\n",
    "                        temp_collection.append(temp[1])\n",
    "                    else:\n",
    "                        query_result = projection(query_result, temp[1])\n",
    "                    temp = temp[3:]\n",
    "                else:\n",
    "                    if query_result == None:\n",
    "                        temp_collection.append(temp[1])\n",
    "                    else:\n",
    "                        query_result = projection(query_result, temp[1])\n",
    "                    temp = temp[2:]\n",
    "                \n",
    "            elif function_name == \"selection\":\n",
    "                if optimization is True:\n",
    "                    if temp[1][0] in updated_collections.keys():\n",
    "                        updated_collections[temp[1][0]] = selection(updated_collections.get(temp[1][0]), str(temp[2]))\n",
    "                    else:\n",
    "                        updated_collections[temp[1][0]] = selection(temp[1], str(temp[2]))\n",
    "                else:\n",
    "                    query_result = selection(temp[1], temp[2])\n",
    "                temp =  temp[3:]\n",
    "            \n",
    "            elif function_name == \"sort\":\n",
    "                if query_result is None: # the query does not do any selections, or joins\n",
    "                    query_result = temp_collection[0][0]\n",
    "                else:\n",
    "                    query_result = sort(query_result, temp[1])\n",
    "                temp = temp[2:]\n",
    "            \n",
    "            else:\n",
    "                return None\n",
    "                \n",
    "    \n",
    "    end = time.time()   \n",
    "    print(\"Took: %s ms\" % (end - start))\n",
    "    return tree_to_collection(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execution_without_optimization(query): \n",
    "    \"\"\" This function executes a query without optimization \"\"\"\n",
    "    start = time.time() \n",
    "    \n",
    "    # Parser \n",
    "    part1 = parse_lexer_part1(query)\n",
    "    part2 = algebra_input(part1)\n",
    "\n",
    "    process_order = part2\n",
    "    \n",
    "    query_result = None\n",
    "    \n",
    "    for q in process_order:\n",
    "        temp = q\n",
    "        temp_collection = []\n",
    "        while temp != []:\n",
    "            function_name = temp[0]\n",
    "\n",
    "            if function_name == 'join':\n",
    "                collection_names = temp[1]\n",
    "                if query_result is None:\n",
    "                    query_result = collection_names[0]\n",
    "                    for i in range(1, len(collection_names)):\n",
    "                        query_result = cartesian_product([query_result, collection_names[i]])\n",
    "                else:\n",
    "                    for i in range(0, len(collection_names)):\n",
    "                        query_result = cartesian_product([query_result, collection_names[i]])\n",
    "                \n",
    "                if temp[2] != \"\":\n",
    "                    full_condition = temp[2]\n",
    "                    if \" or \" in full_condition:\n",
    "                        full_condition = full_condition.split(\"and\")\n",
    "                        total = []\n",
    "                        temp_c = \"\"\n",
    "                        for temp_fc in full_condition:\n",
    "                            if \" or \" in temp_fc:\n",
    "                                total.append(temp_c.rstrip(\" and \"))\n",
    "                                total.append(temp_fc)\n",
    "                                temp_c = \"\"\n",
    "                            else: \n",
    "                                temp_c = temp_c + temp_fc + \" and \"\n",
    "                        for condition_split in total:\n",
    "                            query_result = selection(query_result, condition_split)\n",
    "                    else:\n",
    "                        query_result = selection(query_result, temp[2])\n",
    "                temp = temp[3:]\n",
    "\n",
    "            elif function_name == \"projection\":\n",
    "                if query_result == None:\n",
    "                    temp_collection.append(temp[1])\n",
    "                    temp = temp[2:]\n",
    "                else:\n",
    "                    query_result = projection(query_result, temp[1])\n",
    "                    temp = temp[2:]\n",
    "                \n",
    "            elif function_name == \"selection\":\n",
    "                query_result = selection(temp[1], temp[2])\n",
    "                temp =  temp[3:]\n",
    "            \n",
    "            elif function_name == \"sort\":\n",
    "                if query_result is None:\n",
    "                    query_result = temp_collection[0][0]\n",
    "                    query_result = sort(eval(query_result), temp[1])\n",
    "                else:\n",
    "                    query_result = sort(query_result, temp[1])\n",
    "                temp = temp[2:]\n",
    "            \n",
    "            else:\n",
    "                return None\n",
    "                \n",
    "    \n",
    "    end = time.time()   \n",
    "    print(\"Took: %s ms\" % (end - start))\n",
    "    return tree_to_collection(query_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Database 1: Pokemons**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Query 1**: *Retrieve all pokemon names on alphabetical order that weight 9.5 kg and have weakness Ground and/or Psychic*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Parse query in JSONiq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 0.12381625175476074 ms\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "%%rumble \n",
    "let $pokemon := [\n",
    "    ]\n",
    "let $result := for $pokemon in $pokemon[] \n",
    "    where $pokemon.\"weight\" = \"9.5 kg\" \n",
    "    and jn:members($pokemon.\"weaknesses\") = \"Ground\" or jn:members($pokemon.\"weaknesses\") = \"Psychic\" \n",
    "            order by $pokemon.\"name\" ascending \n",
    "            return {\"name\": $pokemon.\"name\"} \n",
    "        return [$result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Query 1 without optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 0.028985023498535156 ms\n",
      "OUTPUT QUERY 1 WITHOUT OPTIMIZATION:  [{'name': 'Weezing'}]\n",
      "LENGTH OUTPUT QUERY 1 WITHOUT OPTIMIZATION:  1\n"
     ]
    }
   ],
   "source": [
    "# load database \n",
    "f = open('Data/Pokemons.json', )\n",
    "data = json.load(f).get(\"pokemon\")\n",
    "pokemon = json_datamodel(data, \"pokemon\")\n",
    "f.close()\n",
    "\n",
    "query_1 = \"let $result := for $pokemon in $pokemon[] where $pokemon.'weight' = '9.5 kg' and jn:members($pokemon.'weaknesses') = 'Ground' or jn:members($pokemon.'weaknesses') = 'Psychic' order by $pokemon.'name' ascending return {'name': $pokemon.'name'} return [$result]\"\n",
    "result_1 = execution_without_optimization(query_1)\n",
    "print(\"OUTPUT QUERY 1 WITHOUT OPTIMIZATION: \", result_1)\n",
    "print(\"LENGTH OUTPUT QUERY 1 WITHOUT OPTIMIZATION: \", len(result_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Query 1 with optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 0.010460853576660156 ms\n",
      "OUTPUT QUERY 1 WITH OPTIMIZATION:  [{'name': 'Weezing'}]\n",
      "LENGTH OUTPUT QUERY 1 WITH OPTIMIZATION:  1\n"
     ]
    }
   ],
   "source": [
    "f = open('Data/Pokemons.json', )\n",
    "data = json.load(f).get(\"pokemon\")\n",
    "pokemon = json_datamodel(data, \"pokemon\")\n",
    "f.close()\n",
    "\n",
    "query_1 = \"let $result := for $pokemon in $pokemon[] where $pokemon.'weight' = '9.5 kg' and jn:members($pokemon.'weaknesses') = 'Ground' or jn:members(pokemon.'weaknesses') = 'Psychic' order by $pokemon.'name' ascending return {'name': $pokemon.'name'} return [$result]\"\n",
    "result_1 = execution(query_1, True)\n",
    "print(\"OUTPUT QUERY 1 WITH OPTIMIZATION: \", result_1)\n",
    "print(\"LENGTH OUTPUT QUERY 1 WITH OPTIMIZATION: \", len(result_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Query 2**: *Retrieve all Pokemon ids in ascending order that weigh 2.5 kg or more, have height 0.5 m or less and have weakness Electric and/or Flying*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Parse query in JSONiq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 0.770050048828125 ms\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "%%rumble \n",
    "let $pokemon := []\n",
    "let $result := \n",
    "    for $pokemon in $pokemon[] where $pokemon.\"weight\" >= \"2.5 kg\" and $pokemon.\"height\" <= \"0.5 m\"\n",
    "    and jn:members($pokemon.\"weaknesses\") = \"Electric\" or jn:members($pokemon.\"weaknesses\") = \"Flying\"\n",
    "    order by $pokemon.\"id\" ascending return {\"id\": $pokemon.\"id\"} \n",
    "return [$result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Query 2 without optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 0.034896135330200195 ms\n",
      "OUTPUT QUERY 2 WITHOUT OPTIMIZATION:  [{'id': 10}, {'id': 13}, {'id': 46}, {'id': 90}, {'id': 98}, {'id': 102}, {'id': 116}, {'id': 138}]\n",
      "LENGTH OUTPUT QUERY 2 WITHOUT OPTIMIZATION:  8\n"
     ]
    }
   ],
   "source": [
    "f = open('Data/Pokemons.json', )\n",
    "data = json.load(f).get(\"pokemon\")\n",
    "pokemon = json_datamodel(data, \"pokemon\")\n",
    "f.close()\n",
    "\n",
    "query_2 = \"let $result := for $pokemon in $pokemon[] where $pokemon.'weight' >= '2.5 kg' and $pokemon.'height' <= '0.5 m' and  jn:members($pokemon.'weaknesses') = 'Electric' or jn:members($pokemon.'weaknesses') = 'Flying' order by $pokemon.'id' ascending return {'id': $pokemon.'id'} return [$result]\"\n",
    "result_2 = execution_without_optimization(query_2)\n",
    "print(\"OUTPUT QUERY 2 WITHOUT OPTIMIZATION: \", result_2)\n",
    "print(\"LENGTH OUTPUT QUERY 2 WITHOUT OPTIMIZATION: \", len(result_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Query 2 with optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 0.027441740036010742 ms\n",
      "OUTPUT QUERY 2 WITHOUT OPTIMIZATION:  [{'id': 10}, {'id': 13}, {'id': 46}, {'id': 90}, {'id': 98}, {'id': 102}, {'id': 116}, {'id': 138}]\n",
      "LENGTH OUTPUT QUERY 2 WITHOUT OPTIMIZATION:  8\n"
     ]
    }
   ],
   "source": [
    "f = open('Data/Pokemons.json', )\n",
    "data = json.load(f).get(\"pokemon\")\n",
    "pokemon = json_datamodel(data, \"pokemon\")\n",
    "f.close()\n",
    "\n",
    "query_2 = \"let $result := for $pokemon in $pokemon[] where $pokemon.'weight' >= '2.5 kg' and $pokemon.'height' <= '0.5 m' and  jn:members($pokemon.'weaknesses') = 'Electric' or jn:members($pokemon.'weaknesses') = 'Flying' order by $pokemon.'id' ascending return {'id': $pokemon.'id'} return [$result]\"\n",
    "result_2 = execution(query_2, True)\n",
    "print(\"OUTPUT QUERY 2 WITHOUT OPTIMIZATION: \", result_2)\n",
    "print(\"LENGTH OUTPUT QUERY 2 WITHOUT OPTIMIZATION: \", len(result_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Airport flight delays** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Query 3**: *The retrieval of the airport codes in alphabetical order of all airports that experienced more than 200,000 minutes delay in October of 2003*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Parse query in JSONiq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 0.3818988800048828 ms\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "%%rumble \n",
    "let $airport_month := [ ]\n",
    "    \n",
    "let $result := \n",
    "    for $airport_month in $airport_month[]\n",
    "    where $airport_month.\"Statistics\".\"Minutes Delayed\".\"Total\" >= 200000 and $airport_month.\"Time\".\"Month Name\" = \"October\" and $airport_month.\"Time\".\"Year\" = 2003\n",
    "    order by $airport_month.\"Airport\".\"Code\" ascending\n",
    "    return {\"Code\": $airport_month.\"Airport\".\"Code\"}\n",
    "return [$result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Query 3 without optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 0.40004420280456543 ms\n",
      "OUTPUT QUERY 3 WITHOUT OPTIMIZATION:  [{'Code': 'ATL'}, {'Code': 'ORD'}]\n",
      "LENGTH OUTPUT QUERY 3 WITHOUT OPTIMIZATION:  2\n"
     ]
    }
   ],
   "source": [
    "f = open('Data/AirportDelays.json', )\n",
    "data = json.load(f)\n",
    "airportMonth = json_datamodel(data, \"airportMonth\")\n",
    "f.close()\n",
    "\n",
    "query_3 = \"let $result := for $airportMonth in $airportMonth[] where $airportMonth.'Statistics'.'Minutes Delayed'.'Total' >= 200000 and $airportMonth.'Time'.'Month Name' = 'October' and $airportMonth.'Time'.'Year' = 2003 order by $airportMonth.'Airport'.'Code' ascending return {'Code': $airportMonth.'Airport'.'Code'} return [$result]\"\n",
    "result_3 = execution_without_optimization(query_3)\n",
    "print(\"OUTPUT QUERY 3 WITHOUT OPTIMIZATION: \", result_3)\n",
    "print(\"LENGTH OUTPUT QUERY 3 WITHOUT OPTIMIZATION: \", len(result_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Query 3 with optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 0.26115918159484863 ms\n",
      "OUTPUT QUERY 3 WITH OPTIMIZATION:  [{'Code': 'ATL'}, {'Code': 'ORD'}]\n",
      "LENGTH OUTPUT QUERY 3 WITH OPTIMIZATION:  2\n"
     ]
    }
   ],
   "source": [
    "f = open('Data/AirportDelays.json', )\n",
    "data = json.load(f)\n",
    "airportMonth = json_datamodel(data, \"airportMonth\")\n",
    "f.close()\n",
    "\n",
    "query_3 = \"let $result := for $airportMonth in $airportMonth[] where $airportMonth.'Statistics'.'Minutes Delayed'.'Total' >= 200000 and $airportMonth.'Time'.'Month Name' = 'October' and $airportMonth.'Time'.'Year' = 2003 order by $airportMonth.'Airport'.'Code' ascending return {'Code': $airportMonth.'Airport'.'Code'} return [$result]\"\n",
    "result_3 = execution(query_3, True)\n",
    "print(\"OUTPUT QUERY 3 WITH OPTIMIZATION: \", result_3)\n",
    "print(\"LENGTH OUTPUT QUERY 3 WITH OPTIMIZATION: \", len(result_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Query 4**: *The retrieval of the months and years in ascending order, where airport LAX had more than 20 diverted flights while hosting less than 16,000 flights in that month*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Parse query in JSONiq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 0.06972622871398926 ms\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "%%rumble \n",
    "let $airport_month := [ ]\n",
    "    \n",
    "let $result := \n",
    "    for $airport_month in $airport_month[]\n",
    "    where $airport_month.\"Statistics\".\"Flights\".\"Total\" < 16000 \n",
    "    and $airport_month.\"Statistics\".\"Flights\".\"Diverted\" > 20\n",
    "    return {\"Month Name\": $airport_month.\"Time\".\"Month\", \"Year\": $airport_month.\"Time\".\"Year\"}\n",
    "return [$result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Query 4 without optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 0.4059009552001953 ms\n",
      "OUTPUT QUERY 4 WITHOUT OPTIMIZATION:  [{'Month Name': 'November', 'Year': 2009}, {'Month Name': 'Febuary', 'Year': 2010}, {'Month Name': 'November', 'Year': 2010}, {'Month Name': 'Febuary', 'Year': 2011}, {'Month Name': 'Febuary', 'Year': 2015}]\n",
      "LENGTH OUTPUT QUERY 4 WITHOUT OPTIMIZATION:  5\n"
     ]
    }
   ],
   "source": [
    "f = open('Data/AirportDelays.json', )\n",
    "data = json.load(f)\n",
    "airportMonth = json_datamodel(data, \"airportMonth\")\n",
    "f.close()\n",
    "\n",
    "query_4 = \"let $result := for $airportMonth in $airportMonth[] where $airportMonth.'Airport'.'Code'='LAX' and $airportMonth.'Statistics'.'Flights'.'Total' < 16000 and $airportMonth.'Statistics'.'Flights'.'Diverted' > 20 order by $airportMonth.'Time'.'Year' ascending, $airportMonth.'Time'.'Month Name' ascending return {'Month Name': $airportMonth.'Time'.'Month Name', 'Year': $airportMonth.'Time'.'Year'} return [$result]\"\n",
    "result_4 = execution_without_optimization(query_4)\n",
    "print(\"OUTPUT QUERY 4 WITHOUT OPTIMIZATION: \", result_4)\n",
    "print(\"LENGTH OUTPUT QUERY 4 WITHOUT OPTIMIZATION: \", len(result_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Query 4 with optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 0.2571709156036377 ms\n",
      "OUTPUT QUERY 4 WITH OPTIMIZATION:  [{'Month Name': 'November', 'Year': 2009}, {'Month Name': 'Febuary', 'Year': 2010}, {'Month Name': 'November', 'Year': 2010}, {'Month Name': 'Febuary', 'Year': 2011}, {'Month Name': 'Febuary', 'Year': 2015}]\n",
      "LENGTH OUTPUT QUERY 4 WITH OPTIMIZATION:  5\n"
     ]
    }
   ],
   "source": [
    "f = open('Data/AirportDelays.json', )\n",
    "data = json.load(f)\n",
    "airportMonth = json_datamodel(data, \"airportMonth\")\n",
    "f.close()\n",
    "\n",
    "query_4 = \"let $result := for $airportMonth in $airportMonth[] where $airportMonth.'Airport'.'Code'='LAX' and $airportMonth.'Statistics'.'Flights'.'Total' < 16000 and $airportMonth.'Statistics'.'Flights'.'Diverted' > 20 order by $airportMonth.'Time'.'Year' ascending, $airportMonth.'Time'.'Month Name' ascending return {'Month Name': $airportMonth.'Time'.'Month Name', 'Year': $airportMonth.'Time'.'Year'} return [$result]\"\n",
    "result_4 = execution(query_4, True)\n",
    "print(\"OUTPUT QUERY 4 WITH OPTIMIZATION: \", result_4)\n",
    "print(\"LENGTH OUTPUT QUERY 4 WITH OPTIMIZATION: \", len(result_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bike Share Data at Stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Query 5**: *The retrieval of the station ids of all stations in the region with id 80 and that have more than 8 bikes available at the station*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Parse query in JSONiq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 0.07399988174438477 ms\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "%%rumble \n",
    "let $station := [ ]\n",
    "let $status := [ ]\n",
    "let $result := \n",
    "    for $station in $station[], $status in $status[]\n",
    "    where $station.\"station_id\" = $status.\"station_id\" and $status.\"num_bikes_available\" >= 8 and $station.\"region_id\" = \"region_80\"\n",
    "    return {\"station_id\": $station.\"station_id\"}\n",
    "return [$result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Query 5 without optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 1.7056901454925537 ms\n",
      "OUTPUT QUERY 5 WITHOUT OPTIMIZATION:  [{'station_id': 'hub_290'}, {'station_id': 'hub_299'}, {'station_id': 'hub_314'}, {'station_id': 'hub_632'}, {'station_id': 'hub_1273'}, {'station_id': 'hub_1325'}, {'station_id': 'hub_1782'}, {'station_id': 'hub_2325'}, {'station_id': 'hub_3228'}, {'station_id': 'hub_3229'}, {'station_id': 'hub_5418'}]\n",
      "LENGTH OUTPUT QUERY 5 WITHOUT OPTIMIZATION:  11\n"
     ]
    }
   ],
   "source": [
    "f = open('Data/station-information.json', )\n",
    "data = json.load(f).get('data').get('stations')\n",
    "station = json_datamodel(data, \"station\")\n",
    "f.close()\n",
    "f = open('Data/station-status.json', )\n",
    "data = json.load(f).get('data').get('stations')\n",
    "status = json_datamodel(data, \"status\")\n",
    "f.close()\n",
    "\n",
    "query_5 = \"let $result := for $station in $station[], $status in $status[] where $station.'station_id' = $status.'station_id' and $status.'num_bikes_available' >= 8 and $station.'region_id' = 'region_80' return {'station_id': $station.'station_id'} return [$result]\"\n",
    "result_5 = execution_without_optimization(query_5)\n",
    "print(\"OUTPUT QUERY 5 WITHOUT OPTIMIZATION: \", result_5)\n",
    "print(\"LENGTH OUTPUT QUERY 5 WITHOUT OPTIMIZATION: \", len(result_5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Query 5 with optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 0.12353992462158203 ms\n",
      "OUTPUT QUERY 5 WITH OPTIMIZATION:  [{'station_id': 'hub_290'}, {'station_id': 'hub_299'}, {'station_id': 'hub_314'}, {'station_id': 'hub_632'}, {'station_id': 'hub_1273'}, {'station_id': 'hub_1325'}, {'station_id': 'hub_1782'}, {'station_id': 'hub_2325'}, {'station_id': 'hub_3228'}, {'station_id': 'hub_3229'}, {'station_id': 'hub_5418'}]\n",
      "LENGTH OUTPUT QUERY 5 WITH OPTIMIZATION:  11\n"
     ]
    }
   ],
   "source": [
    "f = open('Data/station-information.json', )\n",
    "data = json.load(f).get('data').get('stations')\n",
    "station = json_datamodel(data, \"station\")\n",
    "f.close()\n",
    "f = open('Data/station-status.json', )\n",
    "data = json.load(f).get('data').get('stations')\n",
    "status = json_datamodel(data, \"status\")\n",
    "f.close()\n",
    "\n",
    "query_5 = \"let $result := for $station in $station[], $status in $status[] where $station.'station_id' = $status.'station_id' and $status.'num_bikes_available' >= 8 and $station.'region_id' = 'region_80' return {'station_id': $station.'station_id'} return [$result]\"\n",
    "result_5 = execution(query_5, True)\n",
    "print(\"OUTPUT QUERY 5 WITH OPTIMIZATION: \", result_5)\n",
    "print(\"LENGTH OUTPUT QUERY 5 WITH OPTIMIZATION: \", len(result_5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Query 6**: *The retrieval of the station ids and region ids of all stations where the number of available bikes is larger than number of available dockers except for stations in the region 80*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Parse query in JSONiq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 0.08336520195007324 ms\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "%%rumble \n",
    "let $station := [ ]\n",
    "let $status := [ ]\n",
    "let $result := for $station in $station[], $status in $status[] \n",
    "    where $station.\"station_id\" = $status.\"station_id\" \n",
    "    and $status.\"num_bikes_available\" > $status.\"num_docks_available\" \n",
    "    and $station.\"region_id\" != \"region_80\" \n",
    "    return {\"station_id\": $station.\"station_id\", \"region_id\": $station.\"region_id\"} \n",
    "return [$result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Query 6 without optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 2.6799161434173584 ms\n",
      "OUTPUT QUERY 6 WITHOUT OPTIMIZATION:  [{'station_id': 'hub_2456', 'region_id': 'region_81'}, {'station_id': 'hub_2457', 'region_id': 'region_81'}, {'station_id': 'hub_2459', 'region_id': 'region_81'}, {'station_id': 'hub_2460', 'region_id': 'region_81'}, {'station_id': 'hub_2528', 'region_id': 'region_81'}, {'station_id': 'hub_2529', 'region_id': 'region_81'}, {'station_id': 'hub_2555', 'region_id': 'region_81'}, {'station_id': 'hub_2606', 'region_id': 'region_81'}, {'station_id': 'hub_2609', 'region_id': 'region_81'}, {'station_id': 'hub_2610', 'region_id': 'region_81'}, {'station_id': 'hub_3583', 'region_id': 'region_81'}, {'station_id': 'hub_3779', 'region_id': 'region_81'}]\n",
      "LENGTH OUTPUT QUERY 6 WITHOUT OPTIMIZATION:  12\n"
     ]
    }
   ],
   "source": [
    "f = open('Data/station-information.json', )\n",
    "data = json.load(f).get('data').get('stations')\n",
    "station = json_datamodel(data, \"station\")\n",
    "f.close()\n",
    "f = open('Data/station-status.json', )\n",
    "data = json.load(f).get('data').get('stations')\n",
    "status = json_datamodel(data, \"status\")\n",
    "f.close()\n",
    "\n",
    "query_6 = \"let $result := for $station in $station[], $status in $status[] where $station.'station_id' = $status.'station_id' and $status.'num_bikes_available' > $status.'num_docks_available' and $station.'region_id' != 'region_80' return {'station_id': $station.'station_id', 'region_id': $station.'region_id'} return [$result]\"\n",
    "result_6 = execution_without_optimization(query_6)\n",
    "print(\"OUTPUT QUERY 6 WITHOUT OPTIMIZATION: \", result_6)\n",
    "print(\"LENGTH OUTPUT QUERY 6 WITHOUT OPTIMIZATION: \", len(result_6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Query 6 with optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 0.4665720462799072 ms\n",
      "OUTPUT QUERY 6 WITH OPTIMIZATION:  [{'station_id': 'hub_2456', 'region_id': 'region_81'}, {'station_id': 'hub_2457', 'region_id': 'region_81'}, {'station_id': 'hub_2459', 'region_id': 'region_81'}, {'station_id': 'hub_2460', 'region_id': 'region_81'}, {'station_id': 'hub_2528', 'region_id': 'region_81'}, {'station_id': 'hub_2529', 'region_id': 'region_81'}, {'station_id': 'hub_2555', 'region_id': 'region_81'}, {'station_id': 'hub_2606', 'region_id': 'region_81'}, {'station_id': 'hub_2609', 'region_id': 'region_81'}, {'station_id': 'hub_2610', 'region_id': 'region_81'}, {'station_id': 'hub_3583', 'region_id': 'region_81'}, {'station_id': 'hub_3779', 'region_id': 'region_81'}]\n",
      "LENGTH OUTPUT QUERY 6 WITH OPTIMIZATION:  12\n"
     ]
    }
   ],
   "source": [
    "f = open('Data/station-information.json', )\n",
    "data = json.load(f).get('data').get('stations')\n",
    "station = json_datamodel(data, \"station\")\n",
    "f.close()\n",
    "f = open('Data/station-status.json', )\n",
    "data = json.load(f).get('data').get('stations')\n",
    "status = json_datamodel(data, \"status\")\n",
    "f.close()\n",
    "\n",
    "query_6 = \"let $result := for $station in $station[], $status in $status[] where $station.'station_id' = $status.'station_id' and $status.'num_bikes_available' > $status.'num_docks_available' and $station.'region_id' != 'region_80' return {'station_id': $station.'station_id', 'region_id': $station.'region_id'} return [$result]\"\n",
    "result_6 = execution(query_6, True)\n",
    "print(\"OUTPUT QUERY 6 WITH OPTIMIZATION: \", result_6)\n",
    "print(\"LENGTH OUTPUT QUERY 6 WITH OPTIMIZATION: \", len(result_6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Rick and Morty "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use Case 7**: *Find all episode names where at least one of the characters is neither female nor male* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Parse query in JSONiq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 0.09275698661804199 ms\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "%%rumble \n",
    "let $character := []\n",
    "let $episode := []\n",
    "let $result := for $episode in $episode[], $character in $character[] where $episode.characters.\"url\" = $character.\"url\" where $character.\"gender\" != \"Male\" and not $character.\"gender\" =  \"Female\" return {\"name\": $episode.\"episode\"} return [$result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Query 7 without optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 9.406656980514526 ms\n",
      "OUTPUT QUERY 7: \n",
      "[{'episode': 'S01E01'}, {'episode': 'S01E01'}, {'episode': 'S01E01'}, {'episode': 'S01E01'}, {'episode': 'S01E03'}, {'episode': 'S01E03'}, {'episode': 'S01E03'}, {'episode': 'S01E03'}, {'episode': 'S01E03'}, {'episode': 'S01E03'}, {'episode': 'S01E05'}, {'episode': 'S01E08'}, {'episode': 'S01E08'}, {'episode': 'S01E08'}, {'episode': 'S01E08'}, {'episode': 'S01E08'}, {'episode': 'S01E08'}, {'episode': 'S01E10'}, {'episode': 'S01E10'}, {'episode': 'S01E11'}, {'episode': 'S01E11'}, {'episode': 'S01E11'}, {'episode': 'S01E11'}, {'episode': 'S01E11'}, {'episode': 'S01E11'}, {'episode': 'S01E11'}, {'episode': 'S02E01'}, {'episode': 'S02E02'}, {'episode': 'S02E02'}, {'episode': 'S02E03'}, {'episode': 'S02E03'}, {'episode': 'S02E04'}, {'episode': 'S02E04'}, {'episode': 'S02E05'}, {'episode': 'S02E05'}, {'episode': 'S02E05'}, {'episode': 'S02E05'}, {'episode': 'S02E06'}, {'episode': 'S02E06'}, {'episode': 'S02E07'}, {'episode': 'S02E08'}, {'episode': 'S02E08'}, {'episode': 'S02E08'}, {'episode': 'S02E08'}, {'episode': 'S02E08'}, {'episode': 'S02E08'}, {'episode': 'S02E08'}, {'episode': 'S02E08'}, {'episode': 'S02E08'}, {'episode': 'S02E10'}, {'episode': 'S02E10'}, {'episode': 'S02E10'}, {'episode': 'S02E10'}, {'episode': 'S02E10'}, {'episode': 'S02E10'}, {'episode': 'S03E01'}, {'episode': 'S03E01'}, {'episode': 'S03E03'}, {'episode': 'S03E03'}, {'episode': 'S03E04'}, {'episode': 'S03E04'}, {'episode': 'S03E04'}, {'episode': 'S03E04'}, {'episode': 'S03E04'}, {'episode': 'S03E04'}, {'episode': 'S03E04'}, {'episode': 'S03E05'}, {'episode': 'S03E06'}, {'episode': 'S03E08'}, {'episode': 'S03E09'}, {'episode': 'S03E10'}, {'episode': 'S04E01'}, {'episode': 'S04E01'}, {'episode': 'S04E02'}, {'episode': 'S04E03'}, {'episode': 'S04E03'}, {'episode': 'S04E05'}, {'episode': 'S04E05'}, {'episode': 'S04E06'}, {'episode': 'S04E06'}, {'episode': 'S04E06'}, {'episode': 'S04E09'}, {'episode': 'S04E09'}, {'episode': 'S04E09'}]\n",
      "LENGTH OUTPUT QUERY 7:  84\n"
     ]
    }
   ],
   "source": [
    "f = open('Data/characters.json', )\n",
    "data = json.load(f)\n",
    "character = json_datamodel(data, \"character\")\n",
    "f.close()\n",
    "f = open('Data/episodes.json', )\n",
    "data = json.load(f)\n",
    "episode = json_datamodel(data, \"episode\")\n",
    "f.close()\n",
    "f = open('Data/locations.json', )\n",
    "data = json.load(f)\n",
    "location = json_datamodel(data, \"location\")\n",
    "f.close()\n",
    "\n",
    "query_7 = \"let $result := for $episode in $episode[] for $character in $character[] where $episode.'characters' = $character.'url' where $character.'gender' != 'Male' and not $character.'gender' = 'Female' return {'name': $episode.'episode'} return [$result]\"\n",
    "result_7 = execution_without_optimization(query_7)\n",
    "print(\"OUTPUT QUERY 7: \")\n",
    "print(result_7)\n",
    "print(\"LENGTH OUTPUT QUERY 7: \", len(result_7))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Query 7 with optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 0.7864601612091064 ms\n",
      "OUTPUT QUERY 7: \n",
      "[{'episode': 'S03E10'}, {'episode': 'S01E01'}, {'episode': 'S01E11'}, {'episode': 'S02E08'}, {'episode': 'S03E04'}, {'episode': 'S02E03'}, {'episode': 'S02E08'}, {'episode': 'S02E07'}, {'episode': 'S02E10'}, {'episode': 'S02E01'}, {'episode': 'S03E01'}, {'episode': 'S02E10'}, {'episode': 'S01E10'}, {'episode': 'S03E01'}, {'episode': 'S01E03'}, {'episode': 'S01E03'}, {'episode': 'S01E03'}, {'episode': 'S01E03'}, {'episode': 'S01E03'}, {'episode': 'S01E03'}, {'episode': 'S03E04'}, {'episode': 'S03E04'}, {'episode': 'S02E08'}, {'episode': 'S02E04'}, {'episode': 'S03E05'}, {'episode': 'S01E08'}, {'episode': 'S02E08'}, {'episode': 'S01E08'}, {'episode': 'S01E01'}, {'episode': 'S03E03'}, {'episode': 'S02E10'}, {'episode': 'S02E05'}, {'episode': 'S02E08'}, {'episode': 'S02E10'}, {'episode': 'S02E05'}, {'episode': 'S02E04'}, {'episode': 'S01E08'}, {'episode': 'S02E06'}, {'episode': 'S03E03'}, {'episode': 'S02E08'}, {'episode': 'S01E05'}, {'episode': 'S01E11'}, {'episode': 'S03E04'}, {'episode': 'S01E08'}, {'episode': 'S01E11'}, {'episode': 'S02E02'}, {'episode': 'S02E08'}, {'episode': 'S02E10'}, {'episode': 'S03E04'}, {'episode': 'S02E06'}, {'episode': 'S02E03'}, {'episode': 'S03E08'}, {'episode': 'S01E01'}, {'episode': 'S01E11'}, {'episode': 'S02E05'}, {'episode': 'S03E04'}, {'episode': 'S01E08'}, {'episode': 'S01E08'}, {'episode': 'S01E11'}, {'episode': 'S01E10'}, {'episode': 'S01E01'}, {'episode': 'S01E11'}, {'episode': 'S02E08'}, {'episode': 'S03E04'}, {'episode': 'S01E11'}, {'episode': 'S02E02'}, {'episode': 'S02E05'}, {'episode': 'S02E08'}, {'episode': 'S02E10'}, {'episode': 'S03E06'}, {'episode': 'S03E09'}, {'episode': 'S04E01'}, {'episode': 'S04E01'}, {'episode': 'S04E02'}, {'episode': 'S04E03'}, {'episode': 'S04E03'}, {'episode': 'S04E05'}, {'episode': 'S04E05'}, {'episode': 'S04E06'}, {'episode': 'S04E06'}, {'episode': 'S04E06'}, {'episode': 'S04E09'}, {'episode': 'S04E09'}, {'episode': 'S04E09'}]\n",
      "LENGTH OUTPUT QUERY 7:  84\n"
     ]
    }
   ],
   "source": [
    "f = open('Data/characters.json', )\n",
    "data = json.load(f)\n",
    "character = json_datamodel(data, \"character\")\n",
    "f.close()\n",
    "f = open('Data/episodes.json', )\n",
    "data = json.load(f)\n",
    "episode = json_datamodel(data, \"episode\")\n",
    "f.close()\n",
    "f = open('Data/locations.json', )\n",
    "data = json.load(f)\n",
    "location = json_datamodel(data, \"location\")\n",
    "f.close()\n",
    "\n",
    "query_7 = \"let $result := for $episode in $episode[] for $character in $character[] where $episode.'characters' = $character.'url' where $character.'gender' != 'Male' and not $character.'gender' = 'Female' return {'name': $episode.'episode'} return [$result]\"\n",
    "result_7 = execution(query_7, True)\n",
    "print(\"OUTPUT QUERY 7: \")\n",
    "print(result_7)\n",
    "print(\"LENGTH OUTPUT QUERY 7: \", len(result_7))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use Case 8**: *Find all character names, episode names and location types of charachters that die and are of type human in a specific episode* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Parse query in JSONiq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 0.07622289657592773 ms\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "%%rumble \n",
    "let $character := []\n",
    "let $episode := []\n",
    "let $location := []\n",
    "let $result := for $episode in $episode[], $character in $character[] \n",
    "        where $episode.\"characters\" = $character.\"url\" \n",
    "        for $location in $location[] where $character.location.\"name\"=$location.\"name\" \n",
    "        where $character.\"species\" = \"Human\" and $character.\"status\"=\"Dead\" \n",
    "        return {\"name\": $character.\"name\", \"episode\":$episode.\"episode\", \"location\":$location.\"name\"} \n",
    "return [$result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Query 8 without optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 3516.147355079651 ms\n",
      "OUTPUT QUERY 8: \n",
      "[{'episode': 'S01E01', 'name': 'Earth (C-137)'}, {'episode': 'S01E01', 'name': 'Earth (C-137)'}, {'episode': 'S01E03', 'name': 'Anatomy Park'}, {'episode': 'S01E03', 'name': 'Anatomy Park'}, {'episode': 'S01E03', 'name': 'Anatomy Park'}, {'episode': 'S01E03', 'name': 'Earth (C-137)'}, {'episode': 'S01E05', 'name': 'Earth (C-137)'}, {'episode': 'S01E05', 'name': 'Earth (C-137)'}, {'episode': 'S01E05', 'name': 'Earth (C-137)'}, {'episode': 'S01E06', 'name': 'Earth (C-137)'}, {'episode': 'S01E06', 'name': 'Earth (Replacement Dimension)'}, {'episode': 'S01E06', 'name': 'Earth (Replacement Dimension)'}, {'episode': 'S01E07', 'name': 'Earth (Replacement Dimension)'}, {'episode': 'S01E08', 'name': 'Interdimensional Cable'}, {'episode': 'S01E10', 'name': 'Citadel of Ricks'}, {'episode': 'S01E10', 'name': 'Citadel of Ricks'}, {'episode': 'S01E10', 'name': \"Earth (Evil Rick's Target Dimension)\"}, {'episode': 'S01E10', 'name': 'Citadel of Ricks'}, {'episode': 'S01E10', 'name': 'Citadel of Ricks'}, {'episode': 'S01E10', 'name': 'Citadel of Ricks'}, {'episode': 'S01E11', 'name': 'Earth (Replacement Dimension)'}, {'episode': 'S01E11', 'name': 'Citadel of Ricks'}, {'episode': 'S01E11', 'name': 'NX-5 Planet Remover'}, {'episode': 'S01E11', 'name': 'Citadel of Ricks'}, {'episode': 'S02E01', 'name': 'Earth (Replacement Dimension)'}, {'episode': 'S02E05', 'name': 'Earth (Replacement Dimension)'}, {'episode': 'S02E05', 'name': 'NX-5 Planet Remover'}, {'episode': 'S02E06', 'name': 'Earth (Giant Telepathic Spiders Dimension)'}, {'episode': 'S02E07', 'name': 'Earth (Replacement Dimension)'}, {'episode': 'S02E08', 'name': 'Interdimensional Cable'}, {'episode': 'S02E10', 'name': 'NX-5 Planet Remover'}, {'episode': 'S03E01', 'name': 'Citadel of Ricks'}, {'episode': 'S03E01', 'name': 'Citadel of Ricks'}, {'episode': 'S03E01', 'name': 'Citadel of Ricks'}, {'episode': 'S03E01', 'name': 'Citadel of Ricks'}, {'episode': 'S03E01', 'name': 'Citadel of Ricks'}, {'episode': 'S03E01', 'name': 'Citadel of Ricks'}, {'episode': 'S03E01', 'name': 'NX-5 Planet Remover'}, {'episode': 'S03E01', 'name': 'Citadel of Ricks'}, {'episode': 'S03E01', 'name': 'Citadel of Ricks'}, {'episode': 'S03E01', 'name': 'Citadel of Ricks'}, {'episode': 'S03E01', 'name': 'Citadel of Ricks'}, {'episode': 'S03E01', 'name': 'Citadel of Ricks'}, {'episode': 'S03E01', 'name': 'Citadel of Ricks'}, {'episode': 'S03E02', 'name': 'Post-Apocalyptic Earth'}, {'episode': 'S03E02', 'name': 'Post-Apocalyptic Earth'}, {'episode': 'S03E02', 'name': 'Post-Apocalyptic Earth'}, {'episode': 'S03E02', 'name': 'Post-Apocalyptic Earth'}, {'episode': 'S03E03', 'name': 'Earth (Replacement Dimension)'}, {'episode': 'S03E03', 'name': 'Earth (Replacement Dimension)'}, {'episode': 'S03E04', 'name': \"Worldender's lair\"}, {'episode': 'S03E04', 'name': \"Worldender's lair\"}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E08', 'name': 'Earth (Replacement Dimension)'}, {'episode': 'S04E01', 'name': 'Earth (Fascist Dimension)'}, {'episode': 'S04E01', 'name': 'Earth (Fascist Dimension)'}, {'episode': 'S04E01', 'name': 'Earth (Fascist Dimension)'}, {'episode': 'S04E01', 'name': 'Earth (Replacement Dimension)'}, {'episode': 'S04E04', 'name': 'Draygon'}, {'episode': 'S04E06', 'name': 'Non-Diegetic Alternative Reality'}, {'episode': 'S04E06', 'name': 'Tickets Please Guy Nightmare'}, {'episode': 'S04E06', 'name': 'Tickets Please Guy Nightmare'}, {'episode': 'S04E06', 'name': 'Tickets Please Guy Nightmare'}, {'episode': 'S04E06', 'name': 'Tickets Please Guy Nightmare'}, {'episode': 'S04E06', 'name': 'Story Train'}, {'episode': 'S04E06', 'name': 'Story Train'}, {'episode': 'S04E06', 'name': 'Story Train'}, {'episode': 'S04E08', 'name': 'Merged Universe'}, {'episode': 'S04E08', 'name': 'Merged Universe'}, {'episode': 'S04E10', 'name': 'NX-5 Planet Remover'}]\n",
      "LENGTH OUTPUT QUERY 8:  90\n"
     ]
    }
   ],
   "source": [
    "f = open('Data/characters.json', )\n",
    "data = json.load(f)\n",
    "character = json_datamodel(data, \"character\")\n",
    "f.close()\n",
    "f = open('Data/episodes.json', )\n",
    "data = json.load(f)\n",
    "episode = json_datamodel(data, \"episode\")\n",
    "f.close()\n",
    "f = open('Data/locations.json', )\n",
    "data = json.load(f)\n",
    "location = json_datamodel(data, \"location\")\n",
    "f.close()\n",
    "\n",
    "\n",
    "query_8 = \"let $result := for $episode in $episode[], $character in $character[] where $episode.'characters' = $character.'url' for $location in $location[] where $character.location.'name'=$location.'name' where $character.'species' = 'Human' and $character.'status'='Dead' return {'name': $character.'name', 'episode':'$episode.'episode', 'location':$location.'name'} return [$result]\"\n",
    "result_8 = execution_without_optimization(query_8)\n",
    "print(\"OUTPUT QUERY 8: \")\n",
    "print(result_8)\n",
    "print(\"LENGTH OUTPUT QUERY 8: \", len(result_8))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Query 8 with optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 2.5363292694091797 ms\n",
      "OUTPUT QUERY 8: \n",
      "[{'episode': 'S01E01', 'name': 'Earth (C-137)'}, {'episode': 'S01E01', 'name': 'Earth (C-137)'}, {'episode': 'S01E03', 'name': 'Anatomy Park'}, {'episode': 'S01E03', 'name': 'Anatomy Park'}, {'episode': 'S01E03', 'name': 'Anatomy Park'}, {'episode': 'S01E03', 'name': 'Earth (C-137)'}, {'episode': 'S01E05', 'name': 'Earth (C-137)'}, {'episode': 'S01E05', 'name': 'Earth (C-137)'}, {'episode': 'S01E05', 'name': 'Earth (C-137)'}, {'episode': 'S01E06', 'name': 'Earth (C-137)'}, {'episode': 'S01E06', 'name': 'Earth (Replacement Dimension)'}, {'episode': 'S01E06', 'name': 'Earth (Replacement Dimension)'}, {'episode': 'S01E07', 'name': 'Earth (Replacement Dimension)'}, {'episode': 'S01E08', 'name': 'Interdimensional Cable'}, {'episode': 'S01E10', 'name': 'Citadel of Ricks'}, {'episode': 'S01E10', 'name': 'Citadel of Ricks'}, {'episode': 'S01E10', 'name': \"Earth (Evil Rick's Target Dimension)\"}, {'episode': 'S01E10', 'name': 'Citadel of Ricks'}, {'episode': 'S01E10', 'name': 'Citadel of Ricks'}, {'episode': 'S01E10', 'name': 'Citadel of Ricks'}, {'episode': 'S01E11', 'name': 'Earth (Replacement Dimension)'}, {'episode': 'S01E11', 'name': 'Citadel of Ricks'}, {'episode': 'S01E11', 'name': 'NX-5 Planet Remover'}, {'episode': 'S01E11', 'name': 'Citadel of Ricks'}, {'episode': 'S02E01', 'name': 'Earth (Replacement Dimension)'}, {'episode': 'S02E05', 'name': 'Earth (Replacement Dimension)'}, {'episode': 'S02E05', 'name': 'NX-5 Planet Remover'}, {'episode': 'S02E06', 'name': 'Earth (Giant Telepathic Spiders Dimension)'}, {'episode': 'S02E07', 'name': 'Earth (Replacement Dimension)'}, {'episode': 'S02E08', 'name': 'Interdimensional Cable'}, {'episode': 'S02E10', 'name': 'NX-5 Planet Remover'}, {'episode': 'S03E01', 'name': 'Citadel of Ricks'}, {'episode': 'S03E01', 'name': 'Citadel of Ricks'}, {'episode': 'S03E01', 'name': 'Citadel of Ricks'}, {'episode': 'S03E01', 'name': 'Citadel of Ricks'}, {'episode': 'S03E01', 'name': 'Citadel of Ricks'}, {'episode': 'S03E01', 'name': 'Citadel of Ricks'}, {'episode': 'S03E01', 'name': 'NX-5 Planet Remover'}, {'episode': 'S03E01', 'name': 'Citadel of Ricks'}, {'episode': 'S03E01', 'name': 'Citadel of Ricks'}, {'episode': 'S03E01', 'name': 'Citadel of Ricks'}, {'episode': 'S03E01', 'name': 'Citadel of Ricks'}, {'episode': 'S03E01', 'name': 'Citadel of Ricks'}, {'episode': 'S03E01', 'name': 'Citadel of Ricks'}, {'episode': 'S03E02', 'name': 'Post-Apocalyptic Earth'}, {'episode': 'S03E02', 'name': 'Post-Apocalyptic Earth'}, {'episode': 'S03E02', 'name': 'Post-Apocalyptic Earth'}, {'episode': 'S03E02', 'name': 'Post-Apocalyptic Earth'}, {'episode': 'S03E03', 'name': 'Earth (Replacement Dimension)'}, {'episode': 'S03E03', 'name': 'Earth (Replacement Dimension)'}, {'episode': 'S03E04', 'name': \"Worldender's lair\"}, {'episode': 'S03E04', 'name': \"Worldender's lair\"}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E07', 'name': 'Citadel of Ricks'}, {'episode': 'S03E08', 'name': 'Earth (Replacement Dimension)'}, {'episode': 'S04E01', 'name': 'Earth (Fascist Dimension)'}, {'episode': 'S04E01', 'name': 'Earth (Fascist Dimension)'}, {'episode': 'S04E01', 'name': 'Earth (Fascist Dimension)'}, {'episode': 'S04E01', 'name': 'Earth (Replacement Dimension)'}, {'episode': 'S04E04', 'name': 'Draygon'}, {'episode': 'S04E06', 'name': 'Non-Diegetic Alternative Reality'}, {'episode': 'S04E06', 'name': 'Tickets Please Guy Nightmare'}, {'episode': 'S04E06', 'name': 'Tickets Please Guy Nightmare'}, {'episode': 'S04E06', 'name': 'Tickets Please Guy Nightmare'}, {'episode': 'S04E06', 'name': 'Tickets Please Guy Nightmare'}, {'episode': 'S04E06', 'name': 'Story Train'}, {'episode': 'S04E06', 'name': 'Story Train'}, {'episode': 'S04E06', 'name': 'Story Train'}, {'episode': 'S04E08', 'name': 'Merged Universe'}, {'episode': 'S04E08', 'name': 'Merged Universe'}, {'episode': 'S04E10', 'name': 'NX-5 Planet Remover'}]\n",
      "LENGTH OUTPUT QUERY 8:  90\n"
     ]
    }
   ],
   "source": [
    "f = open('Data/characters.json', )\n",
    "data = json.load(f)\n",
    "character = json_datamodel(data, \"character\")\n",
    "f.close()\n",
    "f = open('Data/episodes.json', )\n",
    "data = json.load(f)\n",
    "episode = json_datamodel(data, \"episode\")\n",
    "f.close()\n",
    "f = open('Data/locations.json', )\n",
    "data = json.load(f)\n",
    "location = json_datamodel(data, \"location\")\n",
    "f.close()\n",
    "\n",
    "query_8 = \"let $result := for $episode in $episode[], $character in $character[] where $episode.'characters' = $character.'url' for $location in $location[] where $character.location.'name'=$location.'name' where $character.'species' = 'Human' and $character.'status'='Dead' return {'name': $character.'name', 'episode':'$episode.'episode', 'location':$location.'name'} return [$result]\"\n",
    "result_8 = execution(query_8, True)\n",
    "print(\"OUTPUT QUERY 8: \")\n",
    "print(result_8)\n",
    "print(\"LENGTH OUTPUT QUERY 8: \", len(result_8)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Nobel Prize Winners "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use Case 9**: *The retrieval of the year the Nobel Prize was received, and the first name and surname of all Dutch, German, and Belgium Nobel Prize Laureates that won a Nobel Prize in the category economics, while sorting the result* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Parse query in JSONiq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 0.0864722728729248 ms\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "%%rumble \n",
    "let $prize := []\n",
    "let $laureate := []\n",
    "let $country := []\n",
    "let $result := for $prize in $prize[] \n",
    "    for $laureate in $laureate[] \n",
    "    where $prize.laureates.\"id\"=$laureate.\"id\" and $prize.\"category\" = \"economics\" \n",
    "    where $laureate.\"bornCountryCode\"=\"NL\" or $laureate.\"bornCountryCode\" = \"DE\" or $laureate.\"bornCountryCode\"=\"BE\" \n",
    "    order by $prize.\"year\" descending, $laureate.\"surname\" ascending  \n",
    "    return {\"year\":$prize.\"year\", \"firstname\": $laureate.\"firstname\", \"surname\": $laureate.\"surname\"} \n",
    "return [$result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Query 9 without optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 522.3089599609375 ms\n",
      "OUTPUT QUERY 9: \n",
      "[{'year': '2005', 'firstname': 'Robert J.', 'surname': 'Aumann'}, {'year': '1975', 'firstname': 'Tjalling C.', 'surname': 'Koopmans'}, {'year': '1969', 'firstname': 'Jan', 'surname': 'Tinbergen'}]\n",
      "LENGTH OUTPUT QUERY 9:  3\n"
     ]
    }
   ],
   "source": [
    "f = open('Data/Prizes.json', )\n",
    "data = json.load(f).get(\"prizes\")\n",
    "prize = json_datamodel(data, \"prize\")\n",
    "f.close()\n",
    "f = open('Data/Laureates.json', )\n",
    "data = json.load(f).get(\"laureates\")\n",
    "laureate = json_datamodel(data, \"laureate\")\n",
    "f.close()\n",
    "\n",
    "query_9 = \"let $result := for $prize in $prize[] for $laureate in $laureate[] where $prize.laureates.'id'=$laureate.'id' and $prize.'category' = 'economics' where $laureate.'bornCountryCode'='NL' or $laureate.'bornCountryCode' = 'DE' or $laureate.'bornCountryCode'='BE' order by $prize.'year' descending, $laureate.'surname' ascending  return {'year':$prize.'year', 'firstname': $laureate.'firstname', 'surname': $laureate.'surname'} return [$result]\"\n",
    "result_9 = execution_without_optimization(query_9)\n",
    "print(\"OUTPUT QUERY 9: \")\n",
    "print(result_9)\n",
    "print(\"LENGTH OUTPUT QUERY 9: \", len(result_9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Query 9 with optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 2.0986149311065674 ms\n",
      "OUTPUT QUERY 9: \n",
      "[{'firstname': 'Robert J.', 'surname': 'Aumann', 'year': '2005'}, {'firstname': 'Tjalling C.', 'surname': 'Koopmans', 'year': '1975'}, {'firstname': 'Jan', 'surname': 'Tinbergen', 'year': '1969'}]\n",
      "LENGTH OUTPUT QUERY 9:  3\n"
     ]
    }
   ],
   "source": [
    "f = open('Data/Prizes.json', )\n",
    "data = json.load(f).get(\"prizes\")\n",
    "prize = json_datamodel(data, \"prize\")\n",
    "f.close()\n",
    "f = open('Data/Laureates.json', )\n",
    "data = json.load(f).get(\"laureates\")\n",
    "laureate = json_datamodel(data, \"laureate\")\n",
    "f.close()\n",
    "\n",
    "query_9 = \"let $result := for $prize in $prize[] for $laureate in $laureate[] where $prize.laureates.'id'=$laureate.'id' and $prize.'category' = 'economics' where $laureate.'bornCountryCode'='NL' or $laureate.'bornCountryCode' = 'DE' or $laureate.'bornCountryCode'='BE' order by $prize.'year' descending, $laureate.'surname' ascending  return {'year':$prize.'year', 'firstname': $laureate.'firstname', 'surname': $laureate.'surname'} return [$result]\"\n",
    "result_9 = execution(query_9, True)\n",
    "print(\"OUTPUT QUERY 9: \")\n",
    "print(result_9)\n",
    "print(\"LENGTH OUTPUT QUERY 9: \", len(result_9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use Case 10**: *Retrieve the names of all Dutch Nobel Prize winners* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Parse query in JSONiq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 0.07467889785766602 ms\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "%%rumble \n",
    "let $prize := []\n",
    "let $laureate:= []\n",
    "let $country := []\n",
    "let $result := for $prize in $prize[] \n",
    "        for $laureate in $laureate[]  \n",
    "        where $prize.\"laureates\".\"id\" = $laureate.\"id\"  \n",
    "        for $country in $country[] \n",
    "        where $laureate.\"bornCountryCode\"= $country.\"code\" and $country.\"name\"=\"the Netherlands\" \n",
    "        return {\"firstname\": $laureate.\"firstname\", \"surname\": $laureate.\"surname\"} \n",
    "    return [$result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Query 10 without optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT QUERY 10: \n",
      "\n",
      "LENGTH OUTPUT QUERY 10:  0\n"
     ]
    }
   ],
   "source": [
    "f = open('Data/Prizes.json', )\n",
    "data = json.load(f).get(\"prizes\")\n",
    "prize = json_datamodel(data, \"prize\")\n",
    "f.close()\n",
    "f = open('Data/Laureates.json', )\n",
    "data = json.load(f).get(\"laureates\")\n",
    "laureate = json_datamodel(data, \"laureate\")\n",
    "f.close()\n",
    "f = open('Data/Countries.json', )\n",
    "data = json.load(f).get(\"countries\")\n",
    "country = json_datamodel(data, \"country\")\n",
    "f.close()\n",
    "\n",
    "# DO NOT RUN THIS QUERY AS IT WILL TAKE VERY AND COULD CRASH THIS NOTEBOOK\n",
    "query_10 = \"let $result := for $prize in $prize[] for $laureate in $laureate[]  where $prize.laureates.'id'=$laureate.'id' for $country in $country[] where $laureate.'bornCountryCode'= $country.'code' and $country.'name'='the Netherlands' return {'firstname': $laureate.'firstname', 'surname': $laureate.'surname'} return [$result]\"\n",
    "#result_10 = execution(query_10, False)\n",
    "result_10 = \"\"\n",
    "print(\"OUTPUT QUERY 10: \")\n",
    "print(result_10)\n",
    "print(\"LENGTH OUTPUT QUERY 10: \", len(result_10))               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Query 10 with optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 2.4567532539367676 ms\n",
      "OUTPUT QUERY 10: \n",
      "[{'firstname': 'Hendrik A.', 'surname': 'Lorentz'}, {'firstname': 'Pieter', 'surname': 'Zeeman'}, {'firstname': 'Johannes Diderik', 'surname': 'van der Waals'}, {'firstname': 'Heike', 'surname': 'Kamerlingh Onnes'}, {'firstname': 'Frits', 'surname': 'Zernike'}, {'firstname': 'Nicolaas', 'surname': 'Bloembergen'}, {'firstname': 'Simon', 'surname': 'van der Meer'}, {'firstname': 'Gerardus', 'surname': \"'t Hooft\"}, {'firstname': 'Martinus J.G.', 'surname': 'Veltman'}, {'firstname': 'Jacobus H.', 'surname': \"van 't Hoff\"}, {'firstname': 'Peter', 'surname': 'Debye'}, {'firstname': 'Paul J.', 'surname': 'Crutzen'}, {'firstname': 'Christiaan', 'surname': 'Eijkman'}, {'firstname': 'Nikolaas', 'surname': 'Tinbergen'}, {'firstname': 'Tobias', 'surname': 'Asser'}, {'firstname': 'Jan', 'surname': 'Tinbergen'}, {'firstname': 'Tjalling C.', 'surname': 'Koopmans'}, {'firstname': 'Bernard L.', 'surname': 'Feringa'}]\n",
      "LENGTH OUTPUT QUERY 10:  18\n"
     ]
    }
   ],
   "source": [
    "f = open('Data/Prizes.json', )\n",
    "data = json.load(f).get(\"prizes\")\n",
    "prize = json_datamodel(data, \"prize\")\n",
    "f.close()\n",
    "f = open('Data/Laureates.json', )\n",
    "data = json.load(f).get(\"laureates\")\n",
    "laureate = json_datamodel(data, \"laureate\")\n",
    "f.close()\n",
    "f = open('Data/Countries.json', )\n",
    "data = json.load(f).get(\"countries\")\n",
    "country = json_datamodel(data, \"country\")\n",
    "f.close()\n",
    "\n",
    "query_10 = \"let $result := for $prize in $prize[] for $laureate in $laureate[]  where $prize.laureates.'id'=$laureate.'id' for $country in $country[] where $laureate.'bornCountryCode'= $country.'code' and $country.'name'='the Netherlands' return {'firstname': $laureate.'firstname', 'surname': $laureate.'surname'} return [$result]\"\n",
    "result_10 = execution(query_10, True)\n",
    "print(\"OUTPUT QUERY 10: \")\n",
    "print(result_10)\n",
    "print(\"LENGTH OUTPUT QUERY 10: \", len(result_10))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END NOTEBOOK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
